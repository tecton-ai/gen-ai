{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextualized Prompts \n",
    "This tutorial guides you through creating an LLM generated restaurant recommendation function.\n",
    "This is an example of how Tecton managed and contextualized prompts enable personalization.\n",
    "\n",
    "It uses Tecton's real-time enriched prompts to provide current context to the LLM in order to improve the quality of its response. \n",
    "This tutorial demonstrates both LangChain and LlamaIndex integration with Tecton prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'tecton[rift]==0.10.0b32' gcsfs s3fs --force-reinstall\n",
    "!pip install \"$HOME/Downloads/tecton_utils-0.0.4-py3-none-any.whl\" --force-reinstall\n",
    "\n",
    "!pip install openai -q\n",
    "!pip install langchain-openai -q\n",
    "!pip install langchain -q\n",
    "!pip install langchain_community -q\n",
    "!pip install langchain_core -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tecton Prompt\n",
    "\n",
    "In the following cell you'll create a Tecton Agent with a system prompt that provides instructions to the LLM. The instructions are parameterized with a specific user's data. \n",
    "\n",
    "The agent creation function takes a Tecton feature view as input which is used at run-time to acquire the latest values of the parameters for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton_utils.ai import AgentClient, AgentService, prompt\n",
    "from tecton_utils.core_utils import make_request_source\n",
    "\n",
    "\n",
    "\n",
    "def restaurant_recommender_agent( user_info):    \n",
    "    \n",
    "    location_request = make_request_source(location = str)\n",
    "\n",
    "    @prompt(sources=[ location_request, user_info])\n",
    "    def sys_prompt(location_request, user_info ):\n",
    "        name = user_info[\"name\"]\n",
    "        food_preference = user_info[\"food_preference\"]\n",
    "        location = location_request[\"location\"]\n",
    "        return f\"\"\"\n",
    "        You are a consierge service that recommends restaurants.\n",
    "        You are serving {name}. Address them by name. \n",
    "        Respond to the user query about dining. \n",
    "        If the user asks for a restaurant recommendation respond with a specific restaurant that you know and suggested menu items. \n",
    "        Suggest restaurants that are in {location}. \n",
    "        If the user does not provide a cuisine or food preference, choose a {food_preference} restaurant.\n",
    "        \"\"\"\n",
    "        \n",
    "    return AgentService(\n",
    "        name=\"restaurant_recommender\",\n",
    "        prompts=[ sys_prompt],\n",
    "        tools=[user_info],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example above uses a single feature view as input. Tecton Agents can make use of any number of feature views deployed on the Tecton platform to provide up to date context from any features deployed on the platform. \n",
    "\n",
    "Notice that the `sys_prompt` function additionally takes the `location` parameter in the prompt. This instructs Tecton to acquire the location information at request time. Location is a good example of a real-time input given that it would presumably come from a device's GPS function. A combination of existing feature pipelines and real-time parameters can be used for any prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "In order to keep this notebook self-contained, you will create a mock feature view with some hard-coded data.\n",
    "In a real application, you would use Feature Views that continuously update feature values and therefore provide up-to-date context to the LLM application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tecton import RequestSource\n",
    "from tecton.types import Field, String\n",
    "\n",
    "\n",
    "from tecton_utils.testing import mock_batch_feature_view\n",
    "\n",
    "\n",
    "mock_data = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"user_id\": \"user1\",\n",
    "                \"name\": \"Jim\",\n",
    "                \"age\": 30,\n",
    "                \"food_preference\": \"American\",\n",
    "            },\n",
    "            {\n",
    "                \"user_id\": \"user2\",\n",
    "                \"name\": \"John\",\n",
    "                \"age\": 40,\n",
    "                \"food_preference\": \"Italian\",\n",
    "            },\n",
    "            {\n",
    "                \"user_id\": \"user3\",\n",
    "                \"name\": \"Jane\",\n",
    "                \"age\": 50,\n",
    "                \"food_preference\": \"Chinese\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "\n",
    "user_preference_fv = mock_batch_feature_view(\n",
    "        \"user_info\", mock_data, entity_keys=[\"user_id\"], description=\"User's profile with name, age and food preference.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature view identifies the key `user_id` that is needed to access a user's data, this attribute must be provided when using the feature view in a prompt. \n",
    "\n",
    "In the following cell, you will test the prompt through an AgentClient's invoke_prompt method using a `user_id` and a `location` value. The `user_id` is used to retrieve a specific user's values. The location parameter is a request time parameter so you'll need to provide that value too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Tecton Agent\n",
    "recommender_agent = restaurant_recommender_agent(user_preference_fv )\n",
    "\n",
    "# create a client to invoke with the agent\n",
    "client = AgentClient.from_local( recommender_agent )\n",
    "\n",
    "#test the agent using \"sys_prompt\" prompt\n",
    "print(client.invoke_prompt(\"sys_prompt\", user_id=\"user3\", location=\"Chicago\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporate Contextualized Prompt into a LangChain agent\n",
    "\n",
    "The Tecton AgentClient can be used to create a LangChain agent which will use the enriched prompt to generate a response.\n",
    "In the cell below you will instantiate an LLM model using OpenAI.\n",
    "\n",
    "Obtain an [OpenAI API key](https://platform.openai.com/api-keys) and replace \"your-openai-key\" in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai as oa\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# replace with your key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
    "\n",
    "# instantiate LLM model\n",
    "gpt_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "#create a lang chain agent that uses the system_prompt \n",
    "lc_agent = client.make_langchain_agent(llm=gpt_llm, system_prompt = \"sys_prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it out\n",
    "\n",
    "In the following cells you can see how the response changes based on the `user_id` and the `location` provided resulting in a personalized response for each user and based on their current location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = lc_agent.invoke( \n",
    "    question=\"suggest a restaurant for tonight and tell me why you suggest it\", \n",
    "    context={\"user_id\":\"user1\", \"location\":\"Charlotte, NC\"} \n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = lc_agent.invoke( \n",
    "    question=\"suggest a restaurant for tonight and tell me why you suggest it\", \n",
    "    context={\"user_id\":\"user1\", \"location\":\"New York, NY\"} \n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = lc_agent.invoke( \n",
    "    question=\"suggest a restaurant for tonight and tell me why you suggest it\", \n",
    "    context={\"user_id\":\"user2\", \"location\":\"Charlotte, NC\"} \n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = lc_agent.invoke( \n",
    "    question=\"suggest a restaurant for tonight and tell me why you suggest it\", \n",
    "    context={\"user_id\":\"user3\", \"location\":\"Charlotte, NC\"} \n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Tecton prompts are used to incorporate real-time, streaming and batch features into your generative AI applications, providing a great solution for personalization. In general, it can be used to provide up to date context for any LLM driven function. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tecton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
