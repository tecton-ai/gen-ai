{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextualized Prompts \n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/tecton-ai/gen-ai/blob/main/context-aware-prompt.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" width=\"150\"/>\n",
    "</a>\n",
    "\n",
    "This tutorial guides you through creating an LLM generated restaurant recommendation function.\n",
    "This is an example of how Tecton managed and contextualized prompts enable personalization.\n",
    "\n",
    "It uses Tecton's real-time enriched prompts to provide current context to the LLM in order to improve the quality of its response. \n",
    "This tutorial demonstrates both LangChain and LlamaIndex integration with Tecton prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'tecton-gen-ai[tecton,langchain,llama-index]' langchain-openai llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log in to Tecton\n",
    "Make sure to hit enter after pasting in your authentication token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tecton\n",
    "\n",
    "tecton.login(\"explore.tecton.ai\")\n",
    "tecton.set_validation_mode(\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tecton Prompt\n",
    "\n",
    "In the following cell you'll create a Tecton Agent with a system prompt that provides instructions to the LLM. The instructions are parameterized with a specific user's data. \n",
    "\n",
    "The agent creation function takes a Tecton feature view as input which is used at run-time to acquire the latest values of the parameters for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton_gen_ai.agent import AgentClient, AgentService\n",
    "from tecton_gen_ai.fco import prompt\n",
    "from tecton_gen_ai.core_utils import make_request_source\n",
    "\n",
    "\n",
    "def restaurant_recommender_agent( user_info):    \n",
    "    \n",
    "    location_request = make_request_source(location = str)\n",
    "\n",
    "    @prompt(sources=[ location_request, user_info])\n",
    "    def sys_prompt(location_request, user_info ):\n",
    "        name = user_info[\"name\"]\n",
    "        food_preference = user_info[\"food_preference\"]\n",
    "        location = location_request[\"location\"]\n",
    "        return f\"\"\"\n",
    "        You are a consierge service that recommends restaurants.\n",
    "        You are serving {name}. Address them by name. \n",
    "        Respond to the user query about dining. \n",
    "        If the user asks for a restaurant recommendation respond with a specific restaurant that you know and suggested menu items. \n",
    "        Suggest restaurants that are in {location}. \n",
    "        If the user does not provide a cuisine or food preference, choose a {food_preference} restaurant.\n",
    "        \"\"\"\n",
    "        \n",
    "    return AgentService(\n",
    "        name=\"restaurant_recommender\",\n",
    "        prompts=[ sys_prompt],\n",
    "        tools=[user_info],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example above uses a single feature view as input. Tecton Agents can make use of any number of feature views deployed on the Tecton platform to provide up to date context from any features deployed on the platform. \n",
    "\n",
    "Notice that the `sys_prompt` function additionally takes the `location` parameter in the prompt. This instructs Tecton to acquire the location information at request time. Location is a good example of a real-time input given that it would presumably come from a device's GPS function. A combination of existing feature pipelines and real-time parameters can be used for any prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "In order to keep this notebook self-contained, you will create a mock feature view with some hard-coded data.\n",
    "In a real application, you would use Feature Views that continuously update feature values and therefore provide up-to-date context to the LLM application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tecton import RequestSource\n",
    "from tecton.types import Field, String\n",
    "\n",
    "\n",
    "from tecton_gen_ai.testing import mock_batch_feature_view\n",
    "\n",
    "\n",
    "mock_data = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"user_id\": \"user1\",\n",
    "                \"name\": \"Jim\",\n",
    "                \"age\": 30,\n",
    "                \"food_preference\": \"American\",\n",
    "            },\n",
    "            {\n",
    "                \"user_id\": \"user2\",\n",
    "                \"name\": \"John\",\n",
    "                \"age\": 40,\n",
    "                \"food_preference\": \"Italian\",\n",
    "            },\n",
    "            {\n",
    "                \"user_id\": \"user3\",\n",
    "                \"name\": \"Jane\",\n",
    "                \"age\": 50,\n",
    "                \"food_preference\": \"Chinese\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "\n",
    "user_preference_fv = mock_batch_feature_view(\n",
    "        \"user_info\", mock_data, entity_keys=[\"user_id\"], description=\"User's profile with name, age and food preference.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature view identifies the key `user_id` that is needed to access a user's data, this attribute must be provided when using the feature view in a prompt. \n",
    "\n",
    "In the following cell, you will test the prompt through an AgentClient's invoke_prompt method using a `user_id` and a `location` value. The `user_id` is used to retrieve a specific user's values. The location parameter is a request time parameter so you'll need to provide that value too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        You are a consierge service that recommends restaurants.\n",
      "        You are serving Jane. Address them by name. \n",
      "        Respond to the user query about dining. \n",
      "        If the user asks for a restaurant recommendation respond with a specific restaurant that you know and suggested menu items. \n",
      "        Suggest restaurants that are in Chicago. \n",
      "        If the user does not provide a cuisine or food preference, choose a Chinese restaurant.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# create the Tecton Agent\n",
    "recommender_agent = restaurant_recommender_agent(user_preference_fv )\n",
    "\n",
    "# create a client to invoke with the agent\n",
    "client = AgentClient.from_local( recommender_agent )\n",
    "\n",
    "#test the agent using \"sys_prompt\" prompt\n",
    "print(client.invoke_prompt(\"sys_prompt\", kwargs=dict(user_id=\"user3\", location=\"Chicago\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporate Contextualized Prompt into a LangChain agent\n",
    "\n",
    "The Tecton AgentClient can be used to create a LangChain agent which will use the enriched prompt to generate a response.\n",
    "In the cell below you will instantiate an LLM model using OpenAI.\n",
    "\n",
    "Obtain an [OpenAI API key](https://platform.openai.com/api-keys) and replace \"your-openai-key\" in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai as oa\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# replace with your key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
    "\n",
    "# instantiate LLM model\n",
    "gpt_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "#create a lang chain agent that uses the system_prompt \n",
    "lc_agent = client.make_agent(llm=gpt_llm, system_prompt = \"sys_prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it out\n",
    "\n",
    "In the following cells you can see how the response changes based on the `user_id` and the `location` provided resulting in a personalized response for each user and based on their current location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Jim! I recommend trying out **The Capital Grille** in Charlotte, NC for dinner tonight. \n",
      "\n",
      "This restaurant is known for its upscale American cuisine and an extensive wine list. The atmosphere is elegant, making it perfect for a special night out. \n",
      "\n",
      "I suggest trying their **Dry Aged Porterhouse** or the **Seared Tenderloin with Butter Poached Lobster**. Both dishes are highly praised and are sure to satisfy your taste. For a side, their **Truffle Fries** are a must-try! \n",
      "\n",
      "Enjoy your dining experience!\n"
     ]
    }
   ],
   "source": [
    "with client.set_context({\"user_id\":\"user1\", \"location\":\"Charlotte, NC\"}):\n",
    "    print(lc_agent.invoke({\"input\":\"suggest a restaurant for tonight and tell me why you suggest it\"})[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Jim! I recommend you try \"The Smith,\" which is a popular American restaurant located in New York, NY. \n",
      "\n",
      "The Smith offers a vibrant atmosphere and is known for its delicious comfort food. I suggest trying their famous mac and cheese, the crispy chicken, or the classic cheeseburger. They also have fantastic cocktails if you're in the mood for a drink. \n",
      "\n",
      "It's a great spot for a casual yet enjoyable dining experience. Enjoy your dinner!\n"
     ]
    }
   ],
   "source": [
    "with client.set_context({\"user_id\":\"user1\", \"location\":\"New York, NY\"}):\n",
    "    print(lc_agent.invoke({\"input\":\"suggest a restaurant for tonight and tell me why you suggest it\"})[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi John! I recommend you try **Carbone**, an iconic Italian restaurant in New York, NY. \n",
      "\n",
      "Carbone is known for its classic Italian-American dishes and a vibrant, upscale atmosphere. Their menu features delicious items like the Spicy Rigatoni Vodka, Veal Parmesan, and the famous Tiramisu for dessert. The combination of great food, attentive service, and a lively ambiance makes it a fantastic choice for a night out. Enjoy your dinner!\n"
     ]
    }
   ],
   "source": [
    "with client.set_context({\"user_id\":\"user2\", \"location\":\"New York, NY\"}):\n",
    "    print(lc_agent.invoke({\"input\":\"suggest a restaurant for tonight and tell me why you suggest it\"})[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Jane! I recommend you try \"Lang Van,\" a fantastic Chinese restaurant in Charlotte, NC. \n",
      "\n",
      "Lang Van is known for its authentic flavors and cozy atmosphere. One of their standout dishes is the \"Kung Pao Chicken,\" which is a delightful mix of tender chicken, peanuts, and vegetables in a savory sauce. You might also enjoy their \"Hot and Sour Soup,\" which is perfect for a warm starter. \n",
      "\n",
      "The combination of great food and inviting service makes it a wonderful choice for your dinner tonight! Enjoy!\n"
     ]
    }
   ],
   "source": [
    "with client.set_context({\"user_id\":\"user3\", \"location\":\"Charlotte, NC\"}):\n",
    "    print(lc_agent.invoke({\"input\":\"suggest a restaurant for tonight and tell me why you suggest it\"})[\"output\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporate Contextualized Prompt into a LlamaIndex agent\n",
    "\n",
    "The Tecton AgentClient can also be used to create a LlamaIndex agent which will use the enriched prompt to generate a response.\n",
    "In the cell below you will instantiate an LLM model but this time using LlamaIndex's integration with OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai as oa\n",
    "import os\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# instantiate LLM model\n",
    "gpt_llm = OpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it out\n",
    "\n",
    "In the following cells you can see how the response changes based on the `user_id` and the `location` provided resulting in a personalized response for each user and based on their current location.\n",
    "\n",
    "Notice that the LlamaIndex agent `li_agent`uses the `chat` method vs LangChain's `invoke` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Jim! I recommend you try **The Capital Grille** in Charlotte, NC. \n",
      "\n",
      "This upscale steakhouse is known for its dry-aged steaks and extensive wine list. The ambiance is perfect for a nice evening out, whether it's a special occasion or just a treat for yourself. \n",
      "\n",
      "I suggest trying their **Bone-In Ribeye** or the **Filet Mignon**, paired with their **Truffle Fries**. For dessert, don't miss the **Chocolate Cake**—it's a crowd favorite! \n",
      "\n",
      "Enjoy your dinner!\n"
     ]
    }
   ],
   "source": [
    "#create a llama-index agent that uses the system_prompt \n",
    "li_agent = client.make_agent(llm=gpt_llm, system_prompt = \"sys_prompt\")\n",
    "\n",
    "# context: user1 in Charlotte\n",
    "with client.set_context({\"user_id\":\"user1\", \"location\":\"Charlotte, NC\"}):\n",
    "    print(li_agent.chat(\"suggest a restaurant for tonight and tell me why you suggest it\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Jim! I recommend you try **The Smith** in New York, NY. It's a fantastic American brasserie known for its vibrant atmosphere and delicious comfort food. \n",
      "\n",
      "You might want to try their famous **Mac & Cheese**, which is a crowd favorite, or the **Smith Burger** for a classic taste. If you're in the mood for something lighter, the **Roasted Chicken** is also a great choice. \n",
      "\n",
      "The ambiance is lively, making it perfect for a fun night out. Enjoy your dinner!\n"
     ]
    }
   ],
   "source": [
    "# since llama-index chat is stateful, you should create another instance if there is a change in context\n",
    "li_agent = client.make_agent(llm=gpt_llm, system_prompt = \"sys_prompt\")\n",
    "\n",
    "# context: user1 in New York\n",
    "with client.set_context({\"user_id\":\"user1\", \"location\":\"New York, NY\"}):\n",
    "    print(li_agent.chat(\"suggest a restaurant for tonight and tell me why you suggest it\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I recommend trying **Caffe Siena** in Charlotte, NC. This charming Italian restaurant offers a cozy atmosphere and a menu filled with authentic Italian dishes. \n",
      "\n",
      "You might enjoy their **Fettuccine Alfredo**, which is creamy and rich, or the **Margherita Pizza**, made with fresh mozzarella and basil. For a delightful dessert, don't miss their **Tiramisu**, which is a perfect way to end your meal. \n",
      "\n",
      "Caffe Siena is known for its warm service and inviting ambiance, making it a great choice for a lovely dinner tonight. Enjoy your meal, John!\n"
     ]
    }
   ],
   "source": [
    "# since llama-index chat is stateful, you should create another instance if there is a change in context\n",
    "li_agent = client.make_agent(llm=gpt_llm, system_prompt = \"sys_prompt\")\n",
    "\n",
    "# context: user2 in Charlotte\n",
    "with client.set_context({\"user_id\":\"user2\", \"location\":\"Charlotte, NC\"}):\n",
    "    print(li_agent.chat(\"suggest a restaurant for tonight and tell me why you suggest it\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Jane! I recommend you try **Lang Van** in Charlotte, NC. It's a fantastic Chinese restaurant known for its authentic flavors and cozy atmosphere.\n",
      "\n",
      "Here are a few menu items you might enjoy:\n",
      "- **Kung Pao Chicken**: A classic dish with a perfect balance of spicy and savory flavors.\n",
      "- **Beef with Broccoli**: Tender beef stir-fried with fresh broccoli in a delicious sauce.\n",
      "- **Shrimp Fried Rice**: A flavorful and satisfying dish that pairs well with any entrée.\n",
      "\n",
      "Lang Van is well-loved for its quality ingredients and friendly service, making it a great choice for a delightful dining experience tonight! Enjoy your meal!\n"
     ]
    }
   ],
   "source": [
    "# since llama-index chat is stateful, you should create another instance if there is a change in context\n",
    "li_agent = client.make_agent(llm=gpt_llm, system_prompt = \"sys_prompt\")\n",
    "\n",
    "# context: user3 in Charlotte\n",
    "with client.set_context({\"user_id\":\"user3\", \"location\":\"Charlotte, NC\"}):\n",
    "    print(li_agent.chat(\"suggest a restaurant for tonight and tell me why you suggest it\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Tecton prompts are used to incorporate real-time, streaming and batch features into your generative AI applications, providing a great solution for personalization. In general, it can be used to provide up to date context for any LLM driven function and ut provides seamless integration with LangChain and LlamaIndex. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tecton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
