{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextualized Prompts \n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/tecton-ai/gen-ai/blob/main/context-aware-prompt.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" width=\"150\"/>\n",
    "</a>\n",
    "\n",
    "This tutorial guides you through creating an LLM generated restaurant recommendation function.\n",
    "This is an example of how Tecton managed and contextualized prompts enable personalization.\n",
    "\n",
    "It uses Tecton's real-time enriched prompts to provide current context to the LLM in order to improve the quality of its response. \n",
    "This tutorial demonstrates both LangChain and LlamaIndex integration with Tecton prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (0.1.21)\n",
      "Requirement already satisfied: llama-index-llms-openai in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (0.2.0)\n",
      "Collecting tecton-gen-ai[langchain,llama-index,tecton]\n",
      "  Downloading tecton_gen_ai-0.0.1b2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton-gen-ai[langchain,llama-index,tecton]) (4.12.2)\n",
      "Requirement already satisfied: langchain in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton-gen-ai[langchain,llama-index,tecton]) (0.2.13)\n",
      "Requirement already satisfied: langchain-community in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton-gen-ai[langchain,llama-index,tecton]) (0.2.12)\n",
      "Requirement already satisfied: langchain-core in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton-gen-ai[langchain,llama-index,tecton]) (0.2.30)\n",
      "Requirement already satisfied: llama-index in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton-gen-ai[langchain,llama-index,tecton]) (0.11.0)\n",
      "Collecting tecton~=1.0.0rc0 (from tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton])\n",
      "  Downloading tecton-1.0.0rc1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from langchain-openai) (1.40.6)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index-llms-openai) (0.11.0.post1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from langchain-core->tecton-gen-ai[langchain,llama-index,tecton]) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from langchain-core->tecton-gen-ai[langchain,llama-index,tecton]) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from langchain-core->tecton-gen-ai[langchain,llama-index,tecton]) (0.1.99)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from langchain-core->tecton-gen-ai[langchain,llama-index,tecton]) (24.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from langchain-core->tecton-gen-ai[langchain,llama-index,tecton]) (2.9.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from langchain-core->tecton-gen-ai[langchain,llama-index,tecton]) (8.5.0)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (2024.9.0)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (3.2.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (10.4.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (2.32.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (4.66.5)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (1.16.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=21.3.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (24.2.0)\n",
      "Requirement already satisfied: boto3 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (1.35.7)\n",
      "Requirement already satisfied: jinja2~=3.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (3.1.4)\n",
      "Requirement already satisfied: pathspec in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (0.12.1)\n",
      "Requirement already satisfied: pendulum~=2.1 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (2.1.2)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (5.28.0)\n",
      "Requirement already satisfied: pypika~=0.48.9 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (0.48.9)\n",
      "Requirement already satisfied: pytimeparse in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (1.1.8)\n",
      "Requirement already satisfied: pandas>=1.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (2.2.2)\n",
      "Requirement already satisfied: texttable in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (1.7.0)\n",
      "Requirement already satisfied: colorama~=0.4 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (0.4.6)\n",
      "Requirement already satisfied: yaspin<3,>=0.16 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (2.5.0)\n",
      "Requirement already satisfied: pygments>=2.7.4 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (2.18.0)\n",
      "Requirement already satisfied: pytest in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (8.3.2)\n",
      "Requirement already satisfied: click~=8.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (8.1.7)\n",
      "Requirement already satisfied: typeguard~=2.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (2.13.3)\n",
      "Requirement already satisfied: sqlparse in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (0.5.1)\n",
      "Requirement already satisfied: semantic-version in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (2.10.0)\n",
      "Requirement already satisfied: pyarrow<16,>=8 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (15.0.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (74.1.2)\n",
      "Requirement already satisfied: pip in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (24.2)\n",
      "Requirement already satisfied: pex~=2.1 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (2.17.0)\n",
      "Requirement already satisfied: deltalake>=0.17.4 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (0.18.2)\n",
      "Requirement already satisfied: duckdb==1.0.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (1.0.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.7.24)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from langchain->tecton-gen-ai[langchain,llama-index,tecton]) (4.0.3)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from langchain->tecton-gen-ai[langchain,llama-index,tecton]) (0.2.2)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.4.0,>=0.3.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index->tecton-gen-ai[langchain,llama-index,tecton]) (0.3.0)\n",
      "Requirement already satisfied: llama-index-cli<0.4.0,>=0.3.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index->tecton-gen-ai[langchain,llama-index,tecton]) (0.3.0)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.3.0,>=0.2.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index->tecton-gen-ai[langchain,llama-index,tecton]) (0.2.3)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.3.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index->tecton-gen-ai[langchain,llama-index,tecton]) (0.3.0)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index->tecton-gen-ai[langchain,llama-index,tecton]) (0.9.48.post3)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index->tecton-gen-ai[langchain,llama-index,tecton]) (0.2.0)\n",
      "Requirement already satisfied: llama-index-program-openai<0.3.0,>=0.2.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index->tecton-gen-ai[langchain,llama-index,tecton]) (0.2.0)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.3.0,>=0.2.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index->tecton-gen-ai[langchain,llama-index,tecton]) (0.2.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.3.0,>=0.2.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index->tecton-gen-ai[langchain,llama-index,tecton]) (0.2.0)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.2.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index->tecton-gen-ai[langchain,llama-index,tecton]) (0.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (1.9.11)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (3.8)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.2.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (3.21.3)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from deltalake>=0.17.4->tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (0.6)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from jinja2~=3.0->tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (2.1.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core->tecton-gen-ai[langchain,llama-index,tecton]) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core->tecton-gen-ai[langchain,llama-index,tecton]) (3.10.7)\n",
      "Requirement already satisfied: llama-cloud>=0.0.11 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index->tecton-gen-ai[langchain,llama-index,tecton]) (0.0.15)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index->tecton-gen-ai[langchain,llama-index,tecton]) (4.12.3)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index->tecton-gen-ai[langchain,llama-index,tecton]) (4.3.1)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index->tecton-gen-ai[langchain,llama-index,tecton]) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.4.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from llama-index-readers-llama-parse>=0.2.0->llama-index->tecton-gen-ai[langchain,llama-index,tecton]) (0.5.0)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from pandas>=1.0->tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from pandas>=1.0->tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from pandas>=1.0->tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (2024.1)\n",
      "Requirement already satisfied: pytzdata>=2020.1 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from pendulum~=2.1->tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (2020.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from pydantic<3,>=1->langchain-core->tecton-gen-ai[langchain,llama-index,tecton]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.2 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from pydantic<3,>=1->langchain-core->tecton-gen-ai[langchain,llama-index,tecton]) (2.23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (1.26.20)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-openai) (1.0.0)\n",
      "Requirement already satisfied: termcolor<3.0,>=2.3 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from yaspin<3,>=0.16->tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (2.4.0)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.7 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from boto3->tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (1.35.7)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from boto3->tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from boto3->tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (0.10.2)\n",
      "Requirement already satisfied: iniconfig in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from pytest->tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from pytest->tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (1.5.0)\n",
      "Requirement already satisfied: tomli>=1 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from pytest->tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (2.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index->tecton-gen-ai[langchain,llama-index,tecton]) (2.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/tecton/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.0->tecton~=1.0.0rc0->tecton[rift]~=1.0.0rc0; extra == \"tecton\"->tecton-gen-ai[langchain,llama-index,tecton]) (1.16.0)\n",
      "Downloading tecton-1.0.0rc1-py3-none-any.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tecton_gen_ai-0.0.1b2-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: tecton-gen-ai, tecton\n",
      "  Attempting uninstall: tecton\n",
      "    Found existing installation: tecton 0.10.0b32\n",
      "    Uninstalling tecton-0.10.0b32:\n",
      "      Successfully uninstalled tecton-0.10.0b32\n",
      "Successfully installed tecton-1.0.0rc1 tecton-gen-ai-0.0.1b2\n"
     ]
    }
   ],
   "source": [
    "!pip install 'tecton-gen-ai[tecton,langchain,llama-index]' langchain-openai llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log in to Tecton\n",
    "Make sure to hit enter after pasting in your authentication token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-authenticating. Switching from https://community.tecton.ai to explore.tecton.ai\n",
      "Please visit the following link to login and access the authentication code:\n",
      "https://login.tecton.ai/oauth2/default/v1/authorize?response_type=code&client_id=0oazt04n72Fkm3APE357&redirect_uri=https%3A%2F%2Fwww.tecton.ai%2Fauthorization-callback&state=18349514793794041611&scope=openid+offline_access+profile+email&code_challenge_method=S256&code_challenge=9AbuZE72nWzJK7S6jdiwOvy-q_-8e3Yy4NgEXar4nYQ\n",
      "✅ Authentication successful!\n"
     ]
    }
   ],
   "source": [
    "import tecton\n",
    "\n",
    "tecton.login(\"explore.tecton.ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tecton Prompt\n",
    "\n",
    "In the following cell you'll create a Tecton Agent with a system prompt that provides instructions to the LLM. The instructions are parameterized with a specific user's data. \n",
    "\n",
    "The agent creation function takes a Tecton feature view as input which is used at run-time to acquire the latest values of the parameters for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton_gen_ai.agent import AgentClient, AgentService\n",
    "from tecton_gen_ai.fco import prompt\n",
    "from tecton_gen_ai.utils.tecton import make_request_source\n",
    "\n",
    "\n",
    "def restaurant_recommender_agent( user_info):    \n",
    "    \n",
    "    location_request = make_request_source(location = str)\n",
    "\n",
    "    @prompt(sources=[ location_request, user_info])\n",
    "    def sys_prompt(location_request, user_info ):\n",
    "        name = user_info[\"name\"]\n",
    "        food_preference = user_info[\"food_preference\"]\n",
    "        location = location_request[\"location\"]\n",
    "        return f\"\"\"\n",
    "        You are a consierge service that recommends restaurants.\n",
    "        You are serving {name}. Address them by name. \n",
    "        Respond to the user query about dining. \n",
    "        If the user asks for a restaurant recommendation respond with a specific restaurant that you know and suggested menu items. \n",
    "        Suggest restaurants that are in {location}. \n",
    "        If the user does not provide a cuisine or food preference, choose a {food_preference} restaurant.\n",
    "        \"\"\"\n",
    "        \n",
    "    return AgentService(\n",
    "        name=\"restaurant_recommender\",\n",
    "        prompts=[ sys_prompt],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example above uses a single feature view as input. Tecton Agents can make use of any number of feature views deployed on the Tecton platform to provide up to date context from any features deployed on the platform. \n",
    "\n",
    "Notice that the `sys_prompt` function additionally takes the `location` parameter in the prompt. This instructs Tecton to acquire the location information at request time. Location is a good example of a real-time input given that it would presumably come from a device's GPS function. A combination of existing feature pipelines and real-time parameters can be used for any prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "In order to keep this notebook self-contained, you will create a mock feature view with some hard-coded data.\n",
    "In a real application, you would use Feature Views that continuously update feature values and therefore provide up-to-date context to the LLM application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tecton import RequestSource\n",
    "from tecton.types import Field, String\n",
    "\n",
    "\n",
    "from tecton_gen_ai.testing import mock_batch_feature_view\n",
    "\n",
    "\n",
    "mock_data = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"user_id\": \"user1\",\n",
    "                \"name\": \"Jim\",\n",
    "                \"age\": 30,\n",
    "                \"food_preference\": \"American\",\n",
    "            },\n",
    "            {\n",
    "                \"user_id\": \"user2\",\n",
    "                \"name\": \"John\",\n",
    "                \"age\": 40,\n",
    "                \"food_preference\": \"Italian\",\n",
    "            },\n",
    "            {\n",
    "                \"user_id\": \"user3\",\n",
    "                \"name\": \"Jane\",\n",
    "                \"age\": 50,\n",
    "                \"food_preference\": \"Chinese\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "\n",
    "user_preference_fv = mock_batch_feature_view(\n",
    "        \"user_info\", mock_data, entity_keys=[\"user_id\"], description=\"User's profile with name, age and food preference.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature view identifies the key `user_id` that is needed to access a user's data, this attribute must be provided when using the feature view in a prompt. \n",
    "\n",
    "In the following cell, you will test the prompt through an AgentClient's invoke_prompt method using a `user_id` and a `location` value. The `user_id` is used to retrieve a specific user's values. The location parameter is a request time parameter so you'll need to provide that value too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        You are a consierge service that recommends restaurants.\n",
      "        You are serving Jane. Address them by name. \n",
      "        Respond to the user query about dining. \n",
      "        If the user asks for a restaurant recommendation respond with a specific restaurant that you know and suggested menu items. \n",
      "        Suggest restaurants that are in Chicago. \n",
      "        If the user does not provide a cuisine or food preference, choose a Chinese restaurant.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# create the Tecton Agent\n",
    "recommender_agent = restaurant_recommender_agent(user_preference_fv )\n",
    "\n",
    "# create a client to invoke with the agent\n",
    "client = AgentClient.from_local( recommender_agent )\n",
    "\n",
    "#test the agent using \"sys_prompt\" prompt\n",
    "print(client.invoke_prompt(\"sys_prompt\", kwargs=dict(user_id=\"user3\", location=\"Chicago\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporate Contextualized Prompt into a LangChain agent\n",
    "\n",
    "The Tecton AgentClient can be used to create a LangChain agent which will use the enriched prompt to generate a response.\n",
    "In the cell below you will instantiate an LLM model using OpenAI.\n",
    "\n",
    "Obtain an [OpenAI API key](https://platform.openai.com/api-keys) and replace \"your-openai-key\" in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai as oa\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# replace with your key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
    "\n",
    "# instantiate LLM model\n",
    "gpt_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "#create a lang chain agent that uses the system_prompt \n",
    "lc_agent = client.make_agent(llm=gpt_llm, system_prompt = \"sys_prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it out\n",
    "\n",
    "In the following cells you can see how the response changes based on the `user_id` and the `location` provided resulting in a personalized response for each user and based on their current location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Jim! I recommend trying **The Capital Grille** in Charlotte. It's a fantastic American steakhouse known for its dry-aged steaks and extensive wine list. The ambiance is upscale yet comfortable, making it perfect for a nice evening out.\n",
      "\n",
      "For your meal, I suggest starting with their famous **Wagyu Beef Carpaccio** as an appetizer. For the main course, you can't go wrong with the **Bone-In Ribeye** or the **Filet Mignon**. If you're in the mood for something lighter, their **Seared Tenderloin with Butter Poached Lobster** is also a great choice. \n",
      "\n",
      "Finish off your dinner with their **Chocolate Hazelnut Cake** for dessert—it's a delightful way to end your meal! Enjoy your evening!\n"
     ]
    }
   ],
   "source": [
    "with client.set_context({\"user_id\":\"user1\", \"location\":\"Charlotte, NC\"}):\n",
    "    print(lc_agent.invoke({\"input\":\"suggest a restaurant for tonight and tell me why you suggest it\"})[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Jim! I recommend trying \"The Smith\" in New York, NY. It's a lively American brasserie that offers a great atmosphere and a diverse menu. The ambiance is perfect for a night out, whether you're looking for a casual dinner or a more vibrant dining experience.\n",
      "\n",
      "For menu items, I suggest starting with their famous Mac & Cheese or the Crispy Calamari as an appetizer. For the main course, the Roasted Chicken or the Steak Frites are excellent choices. And don't forget to save room for their delicious desserts, especially the Chocolate Cake!\n",
      "\n",
      "The Smith is known for its friendly service and vibrant energy, making it a great choice for a memorable evening. Enjoy your dinner!\n"
     ]
    }
   ],
   "source": [
    "with client.set_context({\"user_id\":\"user1\", \"location\":\"New York, NY\"}):\n",
    "    print(lc_agent.invoke({\"input\":\"suggest a restaurant for tonight and tell me why you suggest it\"})[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi John! I recommend you try **Carbone**, an iconic Italian restaurant in New York, NY. \n",
      "\n",
      "Carbone is known for its classic Italian-American dishes and a vibrant, upscale atmosphere. Their menu features delicious items like the Spicy Rigatoni Vodka, Veal Parmesan, and the famous Tiramisu for dessert. The combination of great food, attentive service, and a lively ambiance makes it a fantastic choice for a night out. Enjoy your dinner!\n"
     ]
    }
   ],
   "source": [
    "with client.set_context({\"user_id\":\"user2\", \"location\":\"New York, NY\"}):\n",
    "    print(lc_agent.invoke({\"input\":\"suggest a restaurant for tonight and tell me why you suggest it\"})[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Jane! I recommend you try \"Lang Van,\" a fantastic Chinese restaurant in Charlotte, NC. \n",
      "\n",
      "Lang Van is known for its authentic flavors and cozy atmosphere. One of their standout dishes is the \"Kung Pao Chicken,\" which is a delightful mix of tender chicken, peanuts, and vegetables in a savory sauce. You might also enjoy their \"Hot and Sour Soup,\" which is perfect for a warm starter. \n",
      "\n",
      "The combination of great food and inviting service makes it a wonderful choice for your dinner tonight! Enjoy!\n"
     ]
    }
   ],
   "source": [
    "with client.set_context({\"user_id\":\"user3\", \"location\":\"Charlotte, NC\"}):\n",
    "    print(lc_agent.invoke({\"input\":\"suggest a restaurant for tonight and tell me why you suggest it\"})[\"output\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporate Contextualized Prompt into a LlamaIndex agent\n",
    "\n",
    "The Tecton AgentClient can also be used to create a LlamaIndex agent which will use the enriched prompt to generate a response.\n",
    "In the cell below you will instantiate an LLM model but this time using LlamaIndex's integration with OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai as oa\n",
    "import os\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# instantiate LLM model\n",
    "gpt_llm = OpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it out\n",
    "\n",
    "In the following cells you can see how the response changes based on the `user_id` and the `location` provided resulting in a personalized response for each user and based on their current location.\n",
    "\n",
    "Notice that the LlamaIndex agent `li_agent`uses the `chat` method vs LangChain's `invoke` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Jim! I recommend you try **The Capital Grille** in Charlotte, NC. \n",
      "\n",
      "This upscale steakhouse is known for its dry-aged steaks and extensive wine list. The ambiance is perfect for a nice evening out, whether it's a special occasion or just a treat for yourself. \n",
      "\n",
      "I suggest trying their **Bone-In Ribeye** or the **Filet Mignon**, paired with their **Truffle Fries**. For dessert, don't miss the **Chocolate Cake**—it's a crowd favorite! \n",
      "\n",
      "Enjoy your dinner!\n"
     ]
    }
   ],
   "source": [
    "#create a llama-index agent that uses the system_prompt \n",
    "li_agent = client.make_agent(llm=gpt_llm, system_prompt = \"sys_prompt\")\n",
    "\n",
    "# context: user1 in Charlotte\n",
    "with client.set_context({\"user_id\":\"user1\", \"location\":\"Charlotte, NC\"}):\n",
    "    print(li_agent.chat(\"suggest a restaurant for tonight and tell me why you suggest it\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Jim! I recommend you try **The Smith** in New York, NY. It's a fantastic American brasserie known for its vibrant atmosphere and delicious comfort food. \n",
      "\n",
      "You might want to try their famous **Mac & Cheese**, which is a crowd favorite, or the **Smith Burger** for a classic taste. If you're in the mood for something lighter, the **Roasted Chicken** is also a great choice. \n",
      "\n",
      "The ambiance is lively, making it perfect for a fun night out. Enjoy your dinner!\n"
     ]
    }
   ],
   "source": [
    "# since llama-index chat is stateful, you should create another instance if there is a change in context\n",
    "li_agent = client.make_agent(llm=gpt_llm, system_prompt = \"sys_prompt\")\n",
    "\n",
    "# context: user1 in New York\n",
    "with client.set_context({\"user_id\":\"user1\", \"location\":\"New York, NY\"}):\n",
    "    print(li_agent.chat(\"suggest a restaurant for tonight and tell me why you suggest it\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I recommend trying **Caffe Siena** in Charlotte, NC. This charming Italian restaurant offers a cozy atmosphere and a menu filled with authentic Italian dishes. \n",
      "\n",
      "You might enjoy their **Fettuccine Alfredo**, which is creamy and rich, or the **Margherita Pizza**, made with fresh mozzarella and basil. For a delightful dessert, don't miss their **Tiramisu**, which is a perfect way to end your meal. \n",
      "\n",
      "Caffe Siena is known for its warm service and inviting ambiance, making it a great choice for a lovely dinner tonight. Enjoy your meal, John!\n"
     ]
    }
   ],
   "source": [
    "# since llama-index chat is stateful, you should create another instance if there is a change in context\n",
    "li_agent = client.make_agent(llm=gpt_llm, system_prompt = \"sys_prompt\")\n",
    "\n",
    "# context: user2 in Charlotte\n",
    "with client.set_context({\"user_id\":\"user2\", \"location\":\"Charlotte, NC\"}):\n",
    "    print(li_agent.chat(\"suggest a restaurant for tonight and tell me why you suggest it\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Jane! I recommend you try **Lang Van** in Charlotte, NC. It's a fantastic Chinese restaurant known for its authentic flavors and cozy atmosphere.\n",
      "\n",
      "Here are a few menu items you might enjoy:\n",
      "- **Kung Pao Chicken**: A classic dish with a perfect balance of spicy and savory flavors.\n",
      "- **Beef with Broccoli**: Tender beef stir-fried with fresh broccoli in a delicious sauce.\n",
      "- **Shrimp Fried Rice**: A flavorful and satisfying dish that pairs well with any entrée.\n",
      "\n",
      "Lang Van is well-loved for its quality ingredients and friendly service, making it a great choice for a delightful dining experience tonight! Enjoy your meal!\n"
     ]
    }
   ],
   "source": [
    "# since llama-index chat is stateful, you should create another instance if there is a change in context\n",
    "li_agent = client.make_agent(llm=gpt_llm, system_prompt = \"sys_prompt\")\n",
    "\n",
    "# context: user3 in Charlotte\n",
    "with client.set_context({\"user_id\":\"user3\", \"location\":\"Charlotte, NC\"}):\n",
    "    print(li_agent.chat(\"suggest a restaurant for tonight and tell me why you suggest it\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Tecton prompts are used to incorporate real-time, streaming and batch features into your generative AI applications, providing a great solution for personalization. In general, it can be used to provide up to date context for any LLM driven function and ut provides seamless integration with LangChain and LlamaIndex. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tecton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
