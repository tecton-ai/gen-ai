{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Tecton to provide context for LangChain applications \n",
    "\n",
    "This tutorial guides you through creating a restaurant recommendation AI function using LangChain within a consumer restaurant rating app. The app allows users to:\n",
    "\n",
    "- Record restaurants they visit.\n",
    "- Rate their dining experiences.\n",
    "\n",
    "Tecton collects user activity data from rating events to build a profile of a user's tastes by building an feature that lists:\n",
    "\n",
    "- Preferred cuisines.\n",
    "- Average star ratings given per cuisine.\n",
    "- Total restaurant visits per cuisine.\n",
    "\n",
    "A feature service that serves this feature on-demand for a given user has also been implemented in this sample Tecton workspace and will be used to enrich a LangChain prompt.  \n",
    "In this tutorial you will: \n",
    "- Test a langchain prompt without user data enrichment.\n",
    "- Enrich a LangChain prompt to provide a personalized recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install langchain components\n",
    "!pip install langchain-openai -q\n",
    "!pip install langchain -q\n",
    "!pip install langchain_community -q\n",
    "!pip install langchain_core -q\n",
    "!pip install openai -q\n",
    "!pip install tecton_client -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a chain \n",
    "\n",
    "Obtain an LLM model to use.\n",
    "\n",
    "The following cell instantiates a LangChain model using OpenAI's GPT-4o-mini. \n",
    "\n",
    "For this step you will need to:\n",
    "- Obtain an [OpenAI API key](https://platform.openai.com/api-keys) and replace \"your-openai-key\" in the following cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# replace with your key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
    "\n",
    "# instantiate chat model\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell shows a prompt without any personalization. Since it does not have information about the user, it only provides a general recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a consierge service that recommends restaurants. \n",
    "        Respond to the user query about dining. \n",
    "        If the user asks for a restaurant recommendation respond with a specific restaurant that you know, and suggested menu items. \n",
    "        User query:{user_query}\"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# test the prompt\n",
    "inputs = {\n",
    "    \"user_query\": \"suggest a restaurant for tonight in Ballantyne area of Charlotte and tell me why you suggest it\"\n",
    "}\n",
    "chain.invoke(inputs).splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your results may vary, the recommendation for me was:\n",
    "\n",
    "    I recommend trying **The Capital Grille** in the Ballantyne area of Charlotte. \n",
    "    The address is **7830 Gateway Village Blvd, Charlotte, NC 28277**.',\n",
    "\n",
    "    The Capital Grille is an upscale steakhouse known for its dry-aged steaks and an extensive wine list. The ambiance is elegant and perfect for a nice dinner out. \n",
    "\n",
    "    I suggest trying the **Bone-In Ribeye** or the **Filet Mignon**, both cooked to perfection. For a delicious side, the **Truffle Fries** or **Lobster Mac 'n' Cheese** are highly recommended. If you're in the mood for something lighter, their **Wedge Salad** is a refreshing choice.\n",
    "\n",
    "    Overall, it’s a great place for a special occasion or a memorable dining experience. Enjoy your evening!\n",
    "\n",
    "Not bad, it provided a good restaurant and a reason it is good in a general sense, but now let's personalize it to the user's tastes based on their rating activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the user's tastes to the prompt\n",
    "\n",
    "In the following cell you change the prompt to include the user's tastes `{cuisines}` and provide instructions for the LLM on how to use that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "personalized_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a consierge service that recommends restaurants. \n",
    "        Respond to the user query about dining. \n",
    "        If the user asks for a restaurant recommendation respond with a specific restaurant that you know and suggested menu items. \n",
    "        Respond to the user query by taking into account the user's dining history. \n",
    "        Show their rating of the cuisine you recommend.\n",
    "        If the user does not provide a cuisine, choose a restaurant that fits a cuisine from their highest average ratings:\n",
    "        User's dining history by cuisine: {cuisines}\n",
    "        User query:{user_query}\"\"\"\n",
    ")\n",
    "\n",
    "personalized_chain = personalized_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Tecton to get the latest context\n",
    "\n",
    "We'll access a Tecton Feature Service to get the user's latest tastes as an array of cuisines. Below is the view of the whole data pipeline that processes `user` and `restaurant` data to produce the `cuisine_service` features you are going to use:\n",
    "\n",
    "![](./cuisines_pipeline.png)\n",
    "\n",
    "`user`, `restaurant` and `ratings` data are used to calculate the latest features creating a user profile, restaurant profile and the user's taste profile. In this example we use the `cuisines_service` to access the user's tastes as an average rating by cuisine over a 5 year window.\n",
    "\n",
    "Tecton offers [multiple APIs to access features](https://docs.tecton.ai/docs/reading-feature-data/reading-feature-data-for-inference). In the cell below you will use the [Python client](https://docs.tecton.ai/docs/reading-feature-data/reading-feature-data-for-inference/reading-online-features-for-inference-using-the-python-client) to interact with Tecton's feature serving engine. \n",
    "\n",
    "You'll need a Tecton API key to replace \"your-api-key\" below.\n",
    "\n",
    "PROBABLY NEED TO ADJUST THESE INSTRUCTIONS ONCE WE PUT THE TUTORIAL ON `explore.tecton.ai`.\n",
    " \n",
    "To obtain a Tecton API key this, you will first need to create a new Service Account and give it access to read features from your workspace.\n",
    "\n",
    "✅ Head to the following URL to create a new service account (replace \"explore\" with your organization's account name in the URL as necessary). Be sure to save the API key!\n",
    "\n",
    "https://community.tecton.ai/app/settings/accounts-and-access/service-accounts?create-service-account=true\n",
    "\n",
    "✅ If you are using explore.tecton.ai, this account will automatically be given the necessary privileges to read features from the \"prod\" workspace. Otherwise, you should give the service account access to read features from your newly created workspace by following these steps:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton_client import TectonClient\n",
    "\n",
    "tecton_client = TectonClient(\n",
    "    url=\"https://community.tecton.ai/\",\n",
    "    api_key=\"your-tecton-api-key\",  # REPLACE WITH YOUR TECTON API KEY\n",
    "    default_workspace_name=\"gen-ai-prompt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In order to retrieve a particular user's information, we'll need their `user_id`. \n",
    "\n",
    "In the next cell you:\n",
    "- Use a hard-coded value for `user_id` to simulate the user's session.\n",
    "- Retrieve the user's data from the `cuisines_service` Feature Service using `get_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve features for \"current\" user\n",
    "user_id = \"a6afb498-b24f-4314-93df-5a5040cf1cb7\"\n",
    "\n",
    "\n",
    "features = tecton_client.get_features(\n",
    "    feature_service_name=\"cuisines_service\", join_key_map={\"user_id\": user_id}\n",
    ").get_features_dict()\n",
    "\n",
    "for f in features[\"user_ratings_and_total_visits_by_cuisine.user_ratings_and_total_visits_by_cuisine\"]:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together with the chain\n",
    "In the following cell, you create a recommendation function that:\n",
    "- Retrieves the user's tastes feature and adds it to the LangChain inputs as the `cuisines`\n",
    "- Invokes the LLM using the now personalized_chain and the user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_personalized_recommendation(langchain_chain, user_id, user_query):\n",
    "    # retrieve user features from Tecton\n",
    "    features = tecton_client.get_features(\n",
    "        feature_service_name=\"cuisines_service\", join_key_map={\"user_id\": user_id}\n",
    "    ).get_features_dict()\n",
    "\n",
    "    # build inputs for chat\n",
    "    inputs = {\n",
    "        \"cuisines\": features[\"user_ratings_and_total_visits_by_cuisine.user_ratings_and_total_visits_by_cuisine\"],\n",
    "        \"user_query\": user_query,\n",
    "    }\n",
    "\n",
    "    # run the chain with inputs\n",
    "    return langchain_chain.invoke(inputs)\n",
    "\n",
    "\n",
    "# test the function\n",
    "get_personalized_recommendation(\n",
    "    personalized_chain,\n",
    "    user_id,\n",
    "    user_query=\"suggest a restaurant for tonight in Ballantyne area of Charlotte and tell me why you suggest it\",\n",
    ").splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output at the time of writing this was:\n",
    "\n",
    "    For a delightful dining experience tonight in the Ballantyne area of Charlotte, I recommend **The Cowfish Sushi Burger Bar**. ',\n",
    "\n",
    "    **Address:** 4720 Holly Crest Ln, Charlotte, NC 28277',\n",
    "    This restaurant offers a unique fusion of American and Japanese cuisines, \n",
    "    which aligns perfectly with your high average ratings in American (4.02) \n",
    "    and your interest in Asian Fusion (3.95). ',\n",
    "\n",
    "\n",
    "    **Suggested Menu Items:**\n",
    "    - **Burgushi Rolls**: A fun combination of sushi and burgers, perfect for a twist on traditional dishes.\n",
    "    - **The Cowfish Burger**: A classic American burger with a variety of toppings to choose from.\n",
    "    - **Lobster and Shrimp Roll**: A delicious sushi option for seafood lovers.\n",
    "    - **Signature Milkshakes**: Don''t miss out on their creative milkshakes for dessert!\n",
    "\n",
    "    I believe you''ll enjoy the vibrant atmosphere and the innovative menu that combines two of your favorite cuisines!\n",
    "\n",
    "It makes a big difference when we are able to provide fresh context to the LLM. It not only \"considered\" the cuisines for the specific user, but it found an option that combines 2 of the user's favorites. Prompt engineering leads to better  instructions for the LLM. Combining the prompt with fresh and relevant data improves of the LLM's response. \n",
    "\n",
    "LangChain provides a great framework on which to build LLM based applications. \n",
    "\n",
    "Tecton enhances the chain by providing relevant information and up to date information for real-time, streaming and batch data. This application could be further personalized by taking into account more user information in the prompt. For example, the user's profile could include their dietary restrictions and only recommend restaurants that have options for them. The app could use their current location and local time to automatically suggest restaurant that are near and suggest a breakfast, lunch or dinner experience based on the time of day. Tecton's ability to transform batch, streaming and real-time data delivering the latest feature values to the app, is a great way of giving your generative AI application the context it needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
