{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e51112",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42db390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tecton_gen_ai.fco import source_as_knowledge, prompt, AgentService, AgentClient\n",
    "from tecton_gen_ai.testing import make_local_source, set_dev_mode\n",
    "from tecton_gen_ai.testing.utils import make_local_vector_db_config\n",
    "\n",
    "set_dev_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6fad3a",
   "metadata": {},
   "source": [
    "# Setup Sample Feature Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9405e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton_gen_ai.testing import make_local_batch_feature_view\n",
    "\n",
    "# user specific money transfer statistics\n",
    "transfer_stats = make_local_batch_feature_view(\n",
    "    \"transfer_stats\",\n",
    "    [\n",
    "        {\"user_id\": 1, \"transfers_in_last_7_days\": 20, \"transfers_in_last_1_year\": 22},\n",
    "        {\"user_id\": 2, \"transfers_in_last_7_days\": 0, \"transfers_in_last_1_year\": 10},\n",
    "    ],\n",
    "    [\"user_id\"],\n",
    "    description = \"User's money transfer stats, abnormal activities could cause account suspension\"  # description for LLM\n",
    ")\n",
    "\n",
    "# user profile info\n",
    "user_profile = make_local_batch_feature_view(\n",
    "    \"user_profile\",\n",
    "    [\n",
    "        {\"user_id\": 1, \"name\": \"Jim\", \"age\": 30, \"account_status\":\"suspended\"},\n",
    "        {\"user_id\": 2, \"name\": \"Mary\", \"age\": 16, \"account_status\":\"active\"},\n",
    "    ],\n",
    "    [\"user_id\"],\n",
    "    description = \"User's name, age and account status\"  # description for LLM\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e4d6b0",
   "metadata": {},
   "source": [
    "## The FAQ Data\n",
    "\n",
    "As expected the FAQ data is text with common user questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ef853",
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_df = pd.read_parquet(\"faq.parquet\")\n",
    "display (faq_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad942c7",
   "metadata": {},
   "source": [
    "## Prepare FAQ knowledge for RAG\n",
    "\n",
    "The FAQ text is loaded into a vector search database to provide answers to user's questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6650684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The source description tells the LLM what kind of information it can retrieve from this knowledge base\n",
    "faq_parquet_data = make_local_source(\n",
    "    \"faq\",\n",
    "    faq_df,\n",
    "    description = \"FAQ for TransferApp users\",  # <<<<<<<<\n",
    "    max_rows = len(faq_df)\n",
    ")\n",
    "\n",
    "# calculate embeddings based on the \"question\" such that LLM can find info it needs based on similar questions\n",
    "faq_knowledge = source_as_knowledge(\n",
    "    faq_parquet_data,  # << parquet file of questions and answers\n",
    "    vector_db_config=make_local_vector_db_config(),\n",
    "    vectorize_column=\"question\" # <<<<<<<<\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28c820f",
   "metadata": {},
   "source": [
    "## Connect to LLM Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac0226-5e03-4231-9e33-10b5250a1d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton_gen_ai.testing.interactive import auto_complete, qna\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai = ChatOpenAI(model = \"gpt-4o-2024-08-06\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7063edc",
   "metadata": {},
   "source": [
    "# Demo Scenario\n",
    "\n",
    "TransferApp is a money transfer company that provides services through a mobile app.  \n",
    "\n",
    "- They have an FAQ that user's can search to find responses to their questions.\n",
    "- They want to create a chatbot that can use FAQ to respond to user questions directly.\n",
    "- They've decided to build a RAG GenAI solution for this purpose.\n",
    "\n",
    "\n",
    "In this demo of Tecton we will show how to build a chatbot using the Tecton GenAI Package.\n",
    "\n",
    "- Tecton GenAI SDK to build two version of the chatbot: \n",
    "    - RAG Solution\n",
    "    - Personalized RAG Solution\n",
    "- We will use Tecton's declarative framework to create:\n",
    "    - prompts \n",
    "    - knowledge base from the FAQ data\n",
    "    - user features as LLM tools to create a personalized experience\n",
    "    - Agent Service to serve the chatbot functionality in TransferApp\n",
    "\n",
    "\n",
    "# RAG Solution Options\n",
    "\n",
    "<img src=\"images/naive_rag.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19038c40",
   "metadata": {},
   "source": [
    "## With Tecton\n",
    "\n",
    "<img src=\"images/rag_diagram2.png\" width=\"1000\"/>\n",
    "\n",
    "## Managed RAG Chatbot with Tecton\n",
    "- The prompt provides instructions to the LLM\n",
    "- Prepare the FAQ Knowledge base\n",
    "- The AgentService serves the GenAI application through a REST endpoint:\n",
    "    - the prompt\n",
    "    - the FAQ knowledge \n",
    "    - manages the LLM interaction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7556b73-dca8-45da-b264-f2a7fcca477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the prompt instructs the LLM to use the FAQ knowledge to answer questions \n",
    "@prompt()\n",
    "def sys_prompt() -> str:\n",
    "    return \"\"\"You are an assistant,\n",
    "    TransferApp is a money transfer service on a mobile application.\n",
    "    You answer questions based on FAQ for TranferApp.\n",
    "    Say you don't know if the questions are not relevant to any questions on FAQ\n",
    "\"\"\"\n",
    "\n",
    "# calculate embeddings based on the \"question\" such that LLM can find info it needs based on similar questions\n",
    "faq_knowledge = source_as_knowledge(\n",
    "    faq_parquet_data,  # << parquet file of questions and answers\n",
    "    vector_db_config=make_local_vector_db_config(),\n",
    "    vectorize_column=\"question\" # <<<<<<<<\n",
    ")\n",
    "\n",
    "rag_service = AgentService(\n",
    "    \"chatbot_context\",\n",
    "    prompts=[sys_prompt],\n",
    "    knowledge=[faq_knowledge]\n",
    ")\n",
    "\n",
    "rag_client = AgentClient.from_local(rag_service)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4d993e",
   "metadata": {},
   "source": [
    "## A Simple RAG chatbot\n",
    "\n",
    "It uses generally applicable knowledge so all users get the same experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0350a6-0560-4afd-9945-8aa4862aa112",
   "metadata": {},
   "outputs": [],
   "source": [
    "qna(rag_client, openai, \"sys_prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c403e3d9",
   "metadata": {},
   "source": [
    " # Adding Customized Context to the Chatbot\n",
    "\n",
    "A more useful chatbot can be created by adding better context and tools for the LLM.\n",
    "\n",
    "<img src=images/personalized_diagram.png width=1000/>\n",
    "\n",
    "\n",
    "## Building a smarter TransferApp chatbot\n",
    "\n",
    "In this example we use two feature pipelines:\n",
    " - **transfer_stats** transfer activity aggregations `transfers_last_7days` & `transfers_last_1year`\n",
    " - **user_profile**  data containing `name`, `age` & `account_status`\n",
    "\n",
    "We build a contextualized prompt to provide a personalized experience.\n",
    "\n",
    "We build an LLM Agent that delivers: \n",
    "- FAQ based knowledge\n",
    "- contextualized prompt \n",
    "- tools to access user specific data : **transfer_stats** & **user_profile** data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71e1b30-9d08-495b-8171-a15999f4aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton_gen_ai.fco import tool\n",
    "\n",
    "\n",
    "# this prompt incorporates user_profile context in the prompt  \n",
    "@prompt(sources=[user_profile])\n",
    "def sys_prompt_fv(user_profile) -> str:\n",
    "    return f\"\"\"\n",
    "            You are an assistant, \n",
    "            You answer questions based on the FAQ for TransferApp.\n",
    "            Say you don't know if the questions are not relevant to any questions on FAQ.\n",
    "            You are serving {user_profile['name']}, whose age is {user_profile['age']}.\n",
    "            Address the user by name.\n",
    "            Consult the user's money transfer stats if needed to respond to their question.\n",
    "    \"\"\"\n",
    "    \n",
    "# a service with user context for personalization\n",
    "service_with_fv = AgentService(\n",
    "    \"chatbot_with_personalization\",\n",
    "    prompts=[sys_prompt_fv],\n",
    "    knowledge=[faq_knowledge],\n",
    "    tools=[transfer_stats, user_profile],  # <<<<<< the LLM can now query this user info as needed\n",
    ")\n",
    "\n",
    "client_with_fv = AgentClient.from_local(service_with_fv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809702c6",
   "metadata": {},
   "source": [
    "## A chatbot with user context\n",
    "\n",
    "The chatbot identifies the user with a `user_id` based on their session.\n",
    "\n",
    "Tecton makes use of the `user_id` to personalize the interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa4a61c-b16d-48ae-beb5-7be831292797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the chatbot has the user identified based on their session\n",
    "qna(client_with_fv, openai, \"sys_prompt_fv\", context={\"user_id\":2}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b346296f",
   "metadata": {},
   "source": [
    "# Tecton Apply\n",
    "\n",
    "`tecton apply` command will deploy:\n",
    "- feature views, \n",
    "- prompts, \n",
    "- knowledge bases \n",
    "- agents\n",
    "\n",
    "What happens when you `tecton apply`:\n",
    "\n",
    "- Data Pipelines are automatically created, scheduled, orchestrated and monitored \n",
    "- Data is kept up to date, for streaming and batch sources\n",
    "- Tecton's Retrieval system provide low-latency serving of the features and agent services\n",
    "- Tecton deploys API endpoint for the AgentService that \n",
    "    - controls authenticated access to the chatbot / genai app\n",
    "    - auto-scales\n",
    "    - real-time inputs and their processing logic is also deployed\n",
    "\n",
    "Output from `tecton apply`:\n",
    "\n",
    "<img src=\"images/tecton-apply.png\" width=\"800\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876eb7c3",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Tecton's Declarative Framework makes it easy to build Generative AI applications\n",
    "- prompts, knowledge bases and personalized tools are just a few lines of code\n",
    "- iterating to do prompt engingeering is fast\n",
    "- knowledge base management is automatic\n",
    "- incorporating personalization through enriched prompts and features as tools is simple\n",
    "\n",
    "The Tecton Platform gets the GenAI app into production fast by automating:\n",
    "- data engineering \n",
    "- versioning\n",
    "- data lineage\n",
    "- governance\n",
    "- orchestration\n",
    "- scheduling\n",
    "- monitoring\n",
    "\n",
    "You focus on the Generative AI behavior you want to produce. Tecton takes care of the rest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
