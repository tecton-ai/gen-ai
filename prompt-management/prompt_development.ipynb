{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI Dynamic Prompt Management \n",
    "\n",
    "Generative AI applications have revolutionized how we interact with technology, but their effectiveness hinges on a critical factor: up-to-date context. In this tutorial, we'll explore why delivering current and relevant information is crucial for AI models to produce accurate, useful, and timely outputs. We'll examine how missing or outdated context can lead to irrelevant or incorrect responses, and diminished user trust. \n",
    "\n",
    "This tutorial focuses on using Tecton to create and manage context enriched generative AI prompts. Tecton provides feature pipelines that deliver the required data freshness for batch, streaming and real-time data flows at scale. In this tutorial we start with an environment where multiple feature pipelines are already deployed:\n",
    "\n",
    "- restaurant features, \n",
    "- user's profile features\n",
    "- user's last 100 restaurants visited\n",
    "- user's ratings by cuisine\n",
    "\n",
    "In this tutorial you will:\n",
    "- create a simple AI chat function to simulate a gen AI application\n",
    "- design a prompt for a restaurant recommendation app\n",
    "- refine the prompt by adding context from real-time features \n",
    "- create a new version of the prompt and setup A/B testing\n",
    "- visualize lineage of each prompt version \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/sergioferragut/Downloads/tecton_utils-0.0.3-py3-none-any.whl\n",
      "Collecting typing-extensions>=4.5 (from tecton-utils==0.0.3)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-extensions, tecton-utils\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: tecton-utils\n",
      "    Found existing installation: tecton_utils 0.0.3\n",
      "    Uninstalling tecton_utils-0.0.3:\n",
      "      Successfully uninstalled tecton_utils-0.0.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradio 4.39.0 requires urllib3~=2.0, but you have urllib3 1.26.19 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tecton-utils-0.0.3 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install 'tecton[rift]==0.10.0b32' gcsfs s3fs --force-reinstall\n",
    "!pip install \"$HOME/Downloads/tecton_utils-0.0.3-py3-none-any.whl\" --force-reinstall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-authenticating. Switching from https://community.tecton.ai to community.tecton.ai\n",
      "Please visit the following link to login and access the authentication code:\n",
      "https://login.tecton.ai/oauth2/default/v1/authorize?response_type=code&client_id=0oawb0ilshCmsYdah357&redirect_uri=https%3A%2F%2Fwww.tecton.ai%2Fauthorization-callback&state=2029167732295708416&scope=openid+offline_access+profile+email&code_challenge_method=S256&code_challenge=ZvW1k16MX8QFb0k-uiQZh2lJbQ4X6efdhbATRbqnL88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The set_validation_mode() method is deprecated and will be removed in a future version. As of Tecton version 1.0, objects are automatically validated on creation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Authentication successful!\n"
     ]
    }
   ],
   "source": [
    "import tecton, pandas as pd\n",
    "from tecton import *\n",
    "from tecton.types import *\n",
    "from datetime import datetime, timedelta\n",
    "from pprint import pprint\n",
    "\n",
    "tecton.login('community.tecton.ai')\n",
    "\n",
    "tecton.set_validation_mode('auto')\n",
    "tecton.conf.set('TECTON_OFFLINE_RETRIEVAL_COMPUTE_MODE', 'rift')\n",
    "tecton.conf.set('TECTON_BATCH_COMPUTE_MODE', 'rift')\n",
    "tecton.conf.set(\"DUCKDB_EXTENSION_REPO\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install openai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/envs/tecton/lib/python3.8/site-packages (1.37.0)\n",
      "Collecting openai\n",
      "  Downloading openai-1.40.2-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/tecton/lib/python3.8/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/tecton/lib/python3.8/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/tecton/lib/python3.8/site-packages (from openai) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.5.0-cp38-cp38-macosx_11_0_arm64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/envs/tecton/lib/python3.8/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/tecton/lib/python3.8/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/tecton/lib/python3.8/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/envs/tecton/lib/python3.8/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/tecton/lib/python3.8/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/tecton/lib/python3.8/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/tecton/lib/python3.8/site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/tecton/lib/python3.8/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/tecton/lib/python3.8/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/envs/tecton/lib/python3.8/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/envs/tecton/lib/python3.8/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Downloading openai-1.40.2-py3-none-any.whl (360 kB)\n",
      "Downloading jiter-0.5.0-cp38-cp38-macosx_11_0_arm64.whl (283 kB)\n",
      "Installing collected packages: jiter, openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.37.0\n",
      "    Uninstalling openai-1.37.0:\n",
      "      Successfully uninstalled openai-1.37.0\n",
      "Successfully installed jiter-0.5.0 openai-1.40.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a simple AI Chat Function\n",
    "\n",
    "The following cell creates a simple gen AI function using GPT-4o. It takes the user prompt and the system prompt and provides a single step chat response.\n",
    "\n",
    "For this step you will need to:\n",
    "- Obtain an [OpenAI API key](https://platform.openai.com/api-keys) and replace \"your-openai-key\" in the following cell.\n",
    "- Retrieve your [OpenAI Organization Id](https://platform.openai.com/settings/organization/general) and replace \"your-organization-id\" in the following cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup simple ai chat\n",
    "import openai as oa\n",
    "\n",
    "openai_api_key = \"your-openai-key\"\n",
    "openai_org_id = \"your-openai-org\"\n",
    "\n",
    "\n",
    "# very simple chat function inspired by _j at https://community.openai.com/t/how-do-i-call-chatgpt-api-with-python-code/554554/2\n",
    "def ai_chat( user_prompt:str, system_prompt: str=\"\" )  :\n",
    "    cl=oa.OpenAI(\n",
    "        api_key = openai_api_key,\n",
    "        organization = openai_org_id\n",
    "        )\n",
    "    \n",
    "    messages =[] \n",
    "    messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt })\n",
    "\n",
    "    cc=cl.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"gpt-4o\")\n",
    "    return \"\".join([s.message.content for s in cc.choices]).replace(\"**\",\"\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open AI Test without Context\n",
    "\n",
    "This first test shows how a system prompt without user context, does not have enough information to provide a good response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How about a delightful meal at The Cheesecake Factory? It's a great choice with a varied menu that has something for everyone. \n",
      "\n",
      "I recommend trying the \"Chicken Madeira,\" which comes with chicken breast, fresh asparagus, mozzarella, and a delectable mushroom Madeira sauce, paired with mashed potatoes for a hearty meal. For dessert, don't miss their famous \"White Chocolate Raspberry Truffle Cheesecake.\"\n",
      "\n",
      "The Cheesecake Factory\n",
      "Address: 1156 West Arbrook Boulevard, Arlington, TX 76015\n",
      "\n",
      "Enjoy your dinner!\n"
     ]
    }
   ],
   "source": [
    "system_context = \"\"\"\n",
    "You are a consierge service that recommends restaurants. \n",
    "Your response always includes at least one specific restaurant recommendation \n",
    "Also suggests menu items from the recommended restaurant. \n",
    "Provide the address for the restaurant you recommend.\n",
    "\"\"\"\n",
    "response = ai_chat(\"what should I eat tonight?\", system_context)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response it gives you will vary but it will likely be nowhere near your location or have any sense of what kind of food you like. \n",
    "I'm located in Charlotte, North Carolina, and the response for me was:\n",
    "```\n",
    "    Il Forno\n",
    "    Address: 500 E Main St, Columbus, OH 43215\n",
    "    ...\n",
    "```\n",
    "\n",
    "Clearly nowhere near me, but I do like Italian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tecton Managed Prompts with Context\n",
    "\n",
    "A Tecton on-demand feature view is a great construct for creating up-to-date context for generative AI applications. It provides:\n",
    "- Readily accessible features to create context with batch, stream and real-time data freshness\n",
    "- Full lineage - providing a source for dependency and impact analysis\n",
    "- Version controlled prompts integrated with your coding repository best practices\n",
    "\n",
    "The following cell creates an example of a feature enriched Tecton Prompt, it adds real-time context by including the user's current location and the time of day to the prompt: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__attrs_attrs__',\n",
       " '__attrs_init__',\n",
       " '__attrs_own_setattr__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_args',\n",
       " '_args_supplement',\n",
       " '_assert_supports_skip_validation',\n",
       " '_batch_compute_mode',\n",
       " '_build_args',\n",
       " '_build_ds_specs_with_derived_schema',\n",
       " '_build_fco_validation_args',\n",
       " '_build_feature_view_schema',\n",
       " '_check_can_query_from_source',\n",
       " '_construct_feature_set_config',\n",
       " '_create_unvalidated_feature_definition',\n",
       " '_derive_push_source_schema',\n",
       " '_derive_schemas',\n",
       " '_derive_schemas_with_specs',\n",
       " '_feature_definition',\n",
       " '_from_spec',\n",
       " '_get_dependent_objects',\n",
       " '_get_dependent_specs',\n",
       " '_get_materialization_status',\n",
       " '_get_serving_status',\n",
       " '_id_proto',\n",
       " '_is_local_object',\n",
       " '_is_valid',\n",
       " '_maybe_derive_view_schema',\n",
       " '_run_automatic_validation',\n",
       " '_source_info',\n",
       " '_spec',\n",
       " '_use_feature_param',\n",
       " '_used_explicit_schema',\n",
       " '_validate',\n",
       " '_validate_dependencies',\n",
       " '_validate_start_and_end_times',\n",
       " 'aggregations',\n",
       " 'batch_schedule',\n",
       " 'batch_trigger',\n",
       " 'cancel_materialization_job',\n",
       " 'created_at',\n",
       " 'defined_in',\n",
       " 'delete_keys',\n",
       " 'description',\n",
       " 'entities',\n",
       " 'feature_start_time',\n",
       " 'get_feature_columns',\n",
       " 'get_features_for_events',\n",
       " 'get_features_in_range',\n",
       " 'get_historical_features',\n",
       " 'get_materialization_job',\n",
       " 'get_online_features',\n",
       " 'get_partial_aggregates',\n",
       " 'get_timestamp_field',\n",
       " 'id',\n",
       " 'info',\n",
       " 'is_batch_trigger_manual',\n",
       " 'join_keys',\n",
       " 'list_materialization_jobs',\n",
       " 'materialization_status',\n",
       " 'max_source_data_delay',\n",
       " 'name',\n",
       " 'online_serving_index',\n",
       " 'owner',\n",
       " 'print_transformation_schema',\n",
       " 'published_features_path',\n",
       " 'run',\n",
       " 'run_transformation',\n",
       " 'sources',\n",
       " 'summary',\n",
       " 'tags',\n",
       " 'test_run',\n",
       " 'transformation_schema',\n",
       " 'transformations',\n",
       " 'trigger_materialization_job',\n",
       " 'url',\n",
       " 'validate',\n",
       " 'wait_for_materialization_job',\n",
       " 'wildcard_join_key',\n",
       " 'with_join_key_map',\n",
       " 'with_name',\n",
       " 'workspace']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings_and_total_visits_by_cuisine_raw = tecton.get_feature_view(name='user_ratings_and_total_visits_by_cuisine_raw', workspace='gen-ai-prompt')\n",
    "#print(user_ratings_and_total_visits_by_cuisine_raw._build_feature_view_schema())\n",
    "dir(user_ratings_and_total_visits_by_cuisine_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton_utils.ai import AgentClient, AgentService, prompt, tool\n",
    "from tecton_utils.testing import mock_feature_view\n",
    "\n",
    "\n",
    "def restaurant_recommender_service():\n",
    "    user_ratings_and_total_visits_by_cuisine_raw = tecton.get_feature_view(name='user_ratings_and_total_visits_by_cuisine_raw', workspace='gen-ai-prompt')\n",
    "\n",
    "    # helper function to zip ratings/visit counts by cuisine\n",
    "    def cuisine_summary ( user_ratings_and_total_visits_by_cuisine_raw): \n",
    "        cuisines = user_ratings_and_total_visits_by_cuisine_raw['cuisine_keys_1825d']\n",
    "        ratings = user_ratings_and_total_visits_by_cuisine_raw['users_average_rating_by_cuisine_last_5y']\n",
    "        visits = user_ratings_and_total_visits_by_cuisine_raw['users_total_visits_by_cuisine_last_5y']\n",
    "        return [{'cuisine': cuisine,'average_rating': rating,'total_visits': visit} for cuisine, rating, visit in zip(cuisines, ratings, visits)]\n",
    "\n",
    "    @prompt(sources=[user_ratings_and_total_visits_by_cuisine_raw])\n",
    "    def sys_prompt(location: str, user_ratings_and_total_visits_by_cuisine_raw):\n",
    "        \n",
    "        cuisines = cuisine_summary(user_ratings_and_total_visits_by_cuisine_raw)\n",
    "\n",
    "        return f\"\"\"You are a consierge service that recommends restaurants. \n",
    "        Your response always includes at least one specific restaurant recommendation and suggests menu items. \n",
    "        Provide the address for the restaurant you recommend.\n",
    "        Recommend places that are close to {location}.\n",
    "        If they don't provide a cuisine choose one based on this data:\n",
    "        {cuisines}\n",
    "        Unless prompted by the user, provide choices appropriate the current local time at {location}\n",
    "        You treat them like a good friend.\n",
    "        \"\"\"\n",
    "\n",
    "    return AgentService(\n",
    "        name=\"test\",\n",
    "        prompts=[sys_prompt],\n",
    "        tools=[ user_ratings_and_total_visits_by_cuisine_raw ],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applying spine-based filtering. Num rows: 1. Cardinality: {'user_id': <pyarrow.Int64Scalar: 1>}. Filter: {'user_id': JoinKeyBoundaries(min='953c6db5-89ea-4189-b286-c662591487c8', max='953c6db5-89ea-4189-b286-c662591487c8')}\n",
      "WARNING:root:Applying spine-based filtering. Num rows: 1. Cardinality: {'user_id': <pyarrow.Int64Scalar: 1>}. Filter: {'user_id': JoinKeyBoundaries(min='953c6db5-89ea-4189-b286-c662591487c8', max='953c6db5-89ea-4189-b286-c662591487c8')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey there! Based on your location near 11779 Ridgeway Park Dr, Charlotte, NC, I’d recommend you check out \n",
      "Midwood Smokehouse\n",
      " for some delicious American barbeque. This place is a local favorite and is fantastic for any time of day.\n",
      "\n",
      "### Midwood Smokehouse\n",
      "\n",
      "Address\n",
      ": 540 Brandywine Rd, Charlotte, NC 28209\n",
      "\n",
      "#### Must-Try Menu Items:\n",
      "- \n",
      "Pulled Pork Plate\n",
      ": Slow-smoked pork shoulder, tender and full of flavor.\n",
      "- \n",
      "Burnt Ends\n",
      ": Smoked brisket cubes caramelized with their signature BBQ sauce.\n",
      "- \n",
      "Jalapeno Cornbread\n",
      ": A perfect side to complement your BBQ.\n",
      "- \n",
      "Banana Pudding\n",
      ": For a delightful sweet ending to your meal.\n",
      "\n",
      "If you're looking for something else, let me know if you have a specific cuisine in mind or any dietary preferences! Enjoy your meal!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = AgentClient.from_local(restaurant_recommender_service())\n",
    "user_id = '953c6db5-89ea-4189-b286-c662591487c8'\n",
    "location = \"11779 Ridgeway Park Dr, Charlotte, NC\"\n",
    "\n",
    "#system_prompt = client.invoke_prompt(\"sys_prompt\", user_id=user_id, location=location)\n",
    "#response = ai_chat(\"what should I eat?\", system_prompt)\n",
    "\n",
    "agent = client.make_langchain_agent(llm=, system_prompt = \"sys_prompt\")\n",
    "agent = client.make_llamaindex_agent(llm=LLAMA INDEX OPEN AI CLIENTY)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(BatchFeatureView)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ODFV for the feature enhanced prompt \n",
    "# add location and calculated time of day as context for LLM\n",
    "\n",
    "from tecton import RequestSource, on_demand_feature_view\n",
    "from tecton.types import String, Timestamp, Float64, Field, Bool, String\n",
    "\n",
    "# request data source obtains the user's current location\n",
    "request_schema = [Field(\"location\", String)]\n",
    "user_recommendation_request = RequestSource(schema=request_schema)\n",
    "\n",
    "# on-demand feature view that constructs the system prompt with location and time as context\n",
    "@on_demand_feature_view(\n",
    "    name=\"restaurant_recommender_prompt:v1\", # variant nomenclature provides versioning\n",
    "    sources=[user_recommendation_request],   # identifies sources of features\n",
    "    mode=\"python\",\n",
    "    schema=[Field(\"system_prompt\", String)],\n",
    "    description=\"Contextual prompt for recommending a restaurant\",\n",
    ")\n",
    "def restaurant_recommender_prompt(user_recommendation_request):\n",
    "    \n",
    "    system_prompt_template = \"\"\"\n",
    "        You are a consierge service that recommends restaurants. \n",
    "        Your response always includes at least one specific restaurant recommendation \n",
    "        Also suggests menu items from the recommended restaurant. \n",
    "        Provide the address for the restaurant you recommend.\n",
    "        Recommend places that are close to {location}.\n",
    "        Unless prompted by the user, provide choices appropriate for this time: {time}\n",
    "        You treat them like a good friend.\"\n",
    "    \"\"\"\n",
    "    # build dynamic context into the prompt\n",
    "    prompt_with_context = system_prompt_template.format(\n",
    "        location=user_recommendation_request[\"location\"],\n",
    "        time=datetime.now().strftime(\"%H:%M\")\n",
    "    )\n",
    "    return {\"system_prompt\": prompt_with_context}\n",
    "\n",
    "restaurant_recommender_prompt.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell you simulate the mobile app's request time data adding the location expressed in \"latitude=X,longitude=Y\". Test it with your own location to see how the recommendation changes.\n",
    "This cell is also an example of how the prompt development process can be tested, by adjusting the prompt in the cell above and iteratively testing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the mobile app obtains GPS location and provides it to the app\n",
    "data = {\"user_recommendation_request\":{\"location\":\"latitude=35.0,longitude=-80.0\"}}\n",
    "\n",
    "#test the prompt generation directly from the SDK\n",
    "prompt = restaurant_recommender_prompt.run_transformation(input_data=data)\n",
    "\n",
    "response = ai_chat(\"what should I eat?\", prompt['system_prompt'])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it recommends a restaurant that is near the location I provided and it also reflects the time of day. In my case, it recommended a place for lunch given that I tested at 11:38AM local time.\n",
    "My result:\n",
    "```\n",
    "\"The Fig Tree Restaurant.\"\n",
    "Address\n",
    ": 1601 E 7th St, Charlotte, NC 28204\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Available Features for Better Context\n",
    "So far you've added real-time context to the prompt by using a request time data source for location and calculated local time in the code of the prompt. In this section we'll add more context about the user's preferences based on the user's own history of restaurant ratings.\n",
    "\n",
    "In the following cell you connect to the Tecton Workspace and list the available feature views. During prompt development you can make use of any of the available feature views to easily provide additional context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list deployed feature views \n",
    "ws = tecton.get_workspace('gen-ai-prompt')\n",
    "ws.list_feature_views()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see access any of the feature views on the platform with `tecton.get_feature_view` and see what specific features are available in the feature view by using the `<feature view>.get_feature_columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the feature view we want from the Tecton workspace\n",
    "user_ratings_and_total_visits_by_cuisine_raw = tecton.get_feature_view(name='user_ratings_and_total_visits_by_cuisine_raw', workspace='gen-ai-prompt')\n",
    "user_ratings_and_total_visits_by_cuisine_raw.get_feature_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Second Version of the Prompt\n",
    "For the next revision of the prompt, you will add the user's ratings by cuisine to the context allowing the application to make recommendations based on the user's preferences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a variant ODFV for feature enhanced prompt with additional knowledge of the user\n",
    "@on_demand_feature_view(\n",
    "    name=\"restaurant_recommender_prompt:v2\", #variant name for versioning\n",
    "    sources=[user_recommendation_request,                   # still including current location \n",
    "             user_ratings_and_total_visits_by_cuisine_raw], # add ratings by cuisine\n",
    "    mode=\"python\",\n",
    "    schema=[Field(\"system_prompt\", String)],\n",
    "    description=\"Contextual prompt for recommending a restaurant\",\n",
    ")\n",
    "def restaurant_recommender_prompt_v2(user_recommendation_request, user_ratings_and_total_visits_by_cuisine_raw):\n",
    "    \n",
    "    system_prompt_template = \"\"\"\n",
    "        You are a consierge service that recommends restaurants. \n",
    "        Your response always includes at least one specific restaurant recommendation and suggests menu items. \n",
    "        Provide the address for the restaurant you recommend.\n",
    "        Recommend places that are close to {location}.\n",
    "        If they don't provide a cuisine choose one based on this data:\n",
    "        {cuisine_data}\n",
    "        Unless prompted by the user, provide choices appropriate for this time: {time}\n",
    "\n",
    "        You treat them like a good friend.\"\n",
    "    \"\"\"\n",
    "\n",
    "    # use preaggregated values from user's rating history\n",
    "    cuisines = user_ratings_and_total_visits_by_cuisine_raw['cuisine_keys_1825d']\n",
    "    ratings = user_ratings_and_total_visits_by_cuisine_raw['users_average_rating_by_cuisine_last_5y']\n",
    "    visits = user_ratings_and_total_visits_by_cuisine_raw['users_total_visits_by_cuisine_last_5y']\n",
    "    \n",
    "    # format the ratings for the prompt\n",
    "    cuisine_ratings = []\n",
    "    for cuisine, rating, visit in zip(cuisines, ratings, visits):\n",
    "        cuisine_ratings.append({\n",
    "            'cuisine': cuisine,\n",
    "            'average_rating': rating,\n",
    "            'total_visits': visit\n",
    "        })\n",
    "    \n",
    "    #build final prompt with location, time and cuisine context\n",
    "    prompt_with_context = system_prompt_template.format(\n",
    "        location = user_recommendation_request[\"location\"],\n",
    "        time = datetime.now().strftime(\"%H:%M\"),\n",
    "        cuisine_data = str(cuisine_ratings)\n",
    "    )\n",
    "\n",
    "    return {\"system_prompt\": prompt_with_context}\n",
    "\n",
    "restaurant_recommender_prompt_v2.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lineage\n",
    "\n",
    "You can take a look at the Tecton UI to see the different metadata and pipelines of each version of the prompt at:\n",
    "\n",
    "[Pipeline view for prompt `restaurant_recommender_prompt:v1`](https://community.tecton.ai/app/repo/gen-ai-prompt/feature-services/prompt_restaurant_recommender%3Av1/pipeline)\n",
    "Screenshot:\n",
    "![](./prompt-v1-pipeline.png)\n",
    "The V1 pipelines shows the dependency on the real-time request data source, the transformations it uses to produce the context enriched prompt and the online/batch service that serves it.\n",
    "\n",
    "\n",
    "[Pipeline view for prompt `restaurant_recommender_prompt:v2`](https://community.tecton.ai/app/repo/gen-ai-prompt/feature-services/prompt_restaurant_recommender%3Av2/pipeline)\n",
    "Screenshot:\n",
    "![](./prompt-v2-pipeline.png)\n",
    "The V2 pipelines starts earlier, showing the retaurants and user ratings activity as sources that are transformed into 3 features that keep the last 5 years average rating by cuisine, it also shows the dependency on the real-time request data source, the transformations it uses to produce the context enriched prompt from both the batch and the request time data source and the online/batch service that serves it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup tecton_client for feature retrieval\n",
    "\n",
    "You will need to create a Tecton client API Key here and use it in the following cell, by replacing, \"your-api-key\" for your key.\n",
    "\n",
    "The following cells install \"tecton_client\" package and instantiate a `tecton_client` object for retrieval from a feature service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tecton_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton_client import TectonClient\n",
    "\n",
    "tecton_client = TectonClient(url=\"https://community.tecton.ai/\", \n",
    "                             api_key = \"your-api-key\"  # REPLACE WITH YOUR TECTON API KEY\n",
    "                             default_workspace_name=\"gen-ai-prompt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Restaurant Recommendation App consuming from the Feature Service\n",
    "\n",
    "The following cell simulates the restaurant recommender app by identifying the `user_entity`, and providing the location information in real-time through `app_data`. It uses the `tecton_client` to request online features from the feature service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate a specific user of the app \n",
    "user_id = '953c6db5-89ea-4189-b286-c662591487c8'\n",
    "user_entity = {'user_id': user_id}\n",
    "\n",
    "# simulate request data from mobile device GPS location\n",
    "app_data = {\"location\" : \"latitude=35.0,longitude=-80.0\"}  \n",
    "\n",
    "features = tecton_client.get_features(\n",
    "    feature_service_name=\"prompt_restaurant_recommender:v2\",\n",
    "    join_key_map=user_entity,\n",
    "    request_context_map=app_data,\n",
    ").get_features_dict()\n",
    "\n",
    "system_prompt = features['restaurant_recommender_prompt:v2.system_prompt']\n",
    "print(\"DYNAMIC SYSTEM PROMPT:\\n================================================\\n\")\n",
    "print(system_prompt)\n",
    "print(\"\\n================================================\\n CHAT RESPONSE:\\n\")\n",
    "\n",
    "response = ai_chat(\"what should I eat?\", system_prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It provides a recommendation based on the cuisine data that we provided. Notice that if the user asks for a specific cuisine, the response changes accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ai_chat(\"I feel like eating Indian food, where should I eat?\", prompt['system_prompt'])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "#testing with time travel\n",
    "chat_events = [\n",
    "        {\"timestamp\":datetime.strptime(\"2021-10-01 09:00:01-05:00\",\"%Y-%m-%d %H:%M:%S%z\"),\"user_id\":user_id, \"user_message\": \"where should I eat?\", \"location\":\"latitude=35.0,longitude=-80.0\"},\n",
    "        {\"timestamp\":datetime.strptime(\"2022-10-01 09:00:01-05:00\",\"%Y-%m-%d %H:%M:%S%z\"),\"user_id\":user_id, \"user_message\": \"where should I eat?\", \"location\":\"latitude=35.0,longitude=-80.0\"},\n",
    "        {\"timestamp\":datetime.strptime(\"2023-10-01 09:00:01-05:00\",\"%Y-%m-%d %H:%M:%S%z\"),\"user_id\":user_id, \"user_message\": \"where should I eat?\", \"location\":\"latitude=35.0,longitude=-80.0\"},\n",
    "        ]\n",
    "events_df = pd.DataFrame(chat_events)\n",
    "\n",
    "ws = tecton.get_workspace(\"gen-ai-prompt\")\n",
    "fs = ws.get_feature_service(\"prompt_restaurant_recommender:v2\")\n",
    "batch_data = fs.get_features_for_events(events_df, timestamp_key=\"timestamp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in batch_data.to_pandas()[\"restaurant_recommender_prompt:v2__system_prompt\"]:\n",
    "    print (prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycon-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
