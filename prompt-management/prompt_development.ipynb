{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI Dynamic Prompt Management \n",
    "\n",
    "Generative AI applications have revolutionized how we interact with technology, but their effectiveness hinges on a critical factor: up-to-date context. In this tutorial, we'll explore why delivering current and relevant information is crucial for AI models to produce accurate, useful, and timely outputs. We'll examine how missing or outdated context can lead to irrelevant or incorrect responses, and diminished user trust. \n",
    "\n",
    "This tutorial focuses on using Tecton to create and manage context enriched generative AI prompts. Tecton provides feature pipelines that deliver the required data freshness for batch, streaming and real-time data flows at scale. You'll start with a Tecton workspace that has multiple feature pipelines are already deployed:\n",
    "\n",
    "- restaurant features, \n",
    "- user's profile features\n",
    "- user's last 100 restaurants visited\n",
    "- user's ratings by cuisine\n",
    "\n",
    "In this tutorial you will:\n",
    "- create a simple AI chat function to simulate a gen AI application\n",
    "- design a prompt for a restaurant recommendation app\n",
    "- refine the prompt by adding context from real-time features \n",
    "- create a new version of the prompt and setup A/B testing\n",
    "- visualize lineage of each prompt version \n",
    "- use time travel to test the prompts against context in the past\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'tecton[rift]==0.9.0' gcsfs s3fs -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tecton, pandas as pd\n",
    "from tecton import *\n",
    "from tecton.types import *\n",
    "from datetime import datetime, timedelta\n",
    "from pprint import pprint\n",
    "\n",
    "tecton.login('community.tecton.ai')\n",
    "\n",
    "tecton.set_validation_mode('auto')\n",
    "tecton.conf.set('TECTON_OFFLINE_RETRIEVAL_COMPUTE_MODE', 'rift')\n",
    "tecton.conf.set('TECTON_BATCH_COMPUTE_MODE', 'rift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install openai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a simple AI Chat Function\n",
    "\n",
    "The following cell creates a simple gen AI function using GPT-4o. It takes the user prompt and the system prompt and provides a single step chat response.\n",
    "\n",
    "For this step you will need to:\n",
    "- Obtain an [OpenAI API key](https://platform.openai.com/api-keys) and replace \"your-openai-key\" in the following cell.\n",
    "- Retrieve your [OpenAI Organization Id](https://platform.openai.com/settings/organization/general) and replace \"your-organization-id\" in the following cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup simple ai chat\n",
    "import openai as oa\n",
    "\n",
    "openai_api_key = \"your-openai-key\"\n",
    "openai_org_id = \"your-organization-id\"\n",
    "\n",
    "# very simple chat function inspired by _j at https://community.openai.com/t/how-do-i-call-chatgpt-api-with-python-code/554554/2\n",
    "def ai_chat( user_prompt:str, system_prompt: str=\"\" )  :\n",
    "    cl=oa.OpenAI(\n",
    "        api_key = openai_api_key,\n",
    "        organization = openai_org_id\n",
    "        )\n",
    "    \n",
    "    messages =[] \n",
    "    messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt })\n",
    "\n",
    "    cc=cl.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"gpt-4o\")\n",
    "    return \"\".join([s.message.content for s in cc.choices]).replace(\"**\",\"\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open AI Test without Context\n",
    "\n",
    "This first test shows how a system prompt without user context, does not have enough information to provide a good response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_context = \"\"\"\n",
    "You are a consierge service that recommends restaurants. \n",
    "Your response always includes at least one specific restaurant recommendation \n",
    "Also suggests menu items from the recommended restaurant. \n",
    "Provide the address for the restaurant you recommend.\n",
    "\"\"\"\n",
    "response = ai_chat(\"what should I eat tonight?\", system_context)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response it gives you will vary but it will likely be nowhere near your location or have any sense of what kind of food you like. \n",
    "I'm located in Charlotte, North Carolina, and the response for me was:\n",
    "```\n",
    "    Il Forno\n",
    "    Address: 500 E Main St, Columbus, OH 43215\n",
    "    ...\n",
    "```\n",
    "\n",
    "Clearly nowhere near me, but I do like Italian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tecton Managed Prompts with Context\n",
    "\n",
    "A Tecton on-demand feature view is a great construct for creating up-to-date context for generative AI applications. It provides:\n",
    "- Readily accessible features to create context with batch, stream and real-time data freshness\n",
    "- Full lineage - providing a source for dependency and impact analysis\n",
    "- Version controlled prompts integrated with your coding repository best practices\n",
    "\n",
    "The following cell creates an example of a feature enriched Tecton Prompt, it adds real-time context by including the user's current location and the time of day to the prompt: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ODFV for the feature enhanced prompt \n",
    "# add location and calculated time of day as context for LLM\n",
    "\n",
    "from tecton import RequestSource, on_demand_feature_view\n",
    "from tecton.types import String, Timestamp, Float64, Field, Bool, String\n",
    "\n",
    "# request data source obtains the user's current location\n",
    "request_schema = [Field(\"location\", String)]\n",
    "user_recommendation_request = RequestSource(schema=request_schema)\n",
    "\n",
    "# on-demand feature view that constructs the system prompt with location and time as context\n",
    "@on_demand_feature_view(\n",
    "    name=\"restaurant_recommender_prompt:v1\", # variant nomenclature provides versioning\n",
    "    sources=[user_recommendation_request],   # identifies sources of features\n",
    "    mode=\"python\",\n",
    "    schema=[Field(\"system_prompt\", String)],\n",
    "    description=\"Contextual prompt for recommending a restaurant\",\n",
    ")\n",
    "def restaurant_recommender_prompt(user_recommendation_request):\n",
    "    \n",
    "    system_prompt_template = \"\"\"\n",
    "        You are a consierge service that recommends restaurants. \n",
    "        Your response always includes at least one specific restaurant recommendation \n",
    "        Also suggests menu items from the recommended restaurant. \n",
    "        Provide the address for the restaurant you recommend.\n",
    "        Recommend places that are close to {location}.\n",
    "        Unless prompted by the user, provide choices appropriate for this time: {time}\n",
    "        You treat them like a good friend.\"\n",
    "    \"\"\"\n",
    "    # build dynamic context into the prompt\n",
    "    prompt_with_context = system_prompt_template.format(\n",
    "        location=user_recommendation_request[\"location\"],\n",
    "        time=datetime.now().strftime(\"%H:%M\")\n",
    "    )\n",
    "    return {\"system_prompt\": prompt_with_context}\n",
    "\n",
    "restaurant_recommender_prompt.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell you simulate the mobile app's request time data adding the location expressed in \"latitude=X,longitude=Y\". Test it with your own location to see how the recommendation changes.\n",
    "This cell is also an example of how the prompt development process can be tested, by adjusting the prompt in the cell above and iteratively testing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the mobile app obtains GPS location and provides it to the app\n",
    "data = {\"user_recommendation_request\":{\"location\":\"latitude=35.0,longitude=-80.0\"}}\n",
    "\n",
    "#test the prompt generation directly from the SDK\n",
    "prompt = restaurant_recommender_prompt.run_transformation(input_data=data)\n",
    "\n",
    "response = ai_chat(\"what should I eat?\", prompt['system_prompt'])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it recommends a restaurant that is near the location I provided and it also reflects the time of day. In my case, it recommended a place for lunch given that I tested at 11:38AM local time.\n",
    "My result:\n",
    "```\n",
    "\"The Fig Tree Restaurant.\"\n",
    "Address\n",
    ": 1601 E 7th St, Charlotte, NC 28204\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Available Features for Better Context\n",
    "So far you've added real-time context to the prompt by using a request time data source for location and calculated local time in the code of the prompt. In this section we'll add more context about the user's preferences based on the user's own history of restaurant ratings.\n",
    "\n",
    "In the following cell you connect to the Tecton Workspace and list the available feature views. During prompt development you can make use of any of the available feature views to easily provide additional context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list deployed feature views \n",
    "ws = tecton.get_workspace('gen-ai-prompt')\n",
    "ws.list_feature_views()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see access any of the feature views on the platform with `tecton.get_feature_view` and see what specific features are available in the feature view by using the `<feature view>.get_feature_columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the feature view we want from the Tecton workspace\n",
    "user_ratings_and_total_visits_by_cuisine_raw = tecton.get_feature_view(name='user_ratings_and_total_visits_by_cuisine_raw', workspace='gen-ai-prompt')\n",
    "user_ratings_and_total_visits_by_cuisine_raw.get_feature_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Second Version of the Prompt\n",
    "For the next revision of the prompt, you will add the user's ratings by cuisine to the context allowing the application to make recommendations based on the user's preferences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a variant ODFV for feature enhanced prompt with additional knowledge of the user\n",
    "@on_demand_feature_view(\n",
    "    name=\"restaurant_recommender_prompt:v2\", #variant name for versioning\n",
    "    sources=[user_recommendation_request,                   # still including current location \n",
    "             user_ratings_and_total_visits_by_cuisine_raw], # add ratings by cuisine\n",
    "    mode=\"python\",\n",
    "    schema=[Field(\"system_prompt\", String)],\n",
    "    description=\"Contextual prompt for recommending a restaurant\",\n",
    ")\n",
    "def restaurant_recommender_prompt_v2(user_recommendation_request, user_ratings_and_total_visits_by_cuisine_raw):\n",
    "    \n",
    "    system_prompt_template = \"\"\"\n",
    "        You are a consierge service that recommends restaurants. \n",
    "        Your response always includes at least one specific restaurant recommendation and suggests menu items. \n",
    "        Provide the address for the restaurant you recommend.\n",
    "        Recommend places that are close to {location}.\n",
    "        If they don't provide a cuisine choose one based on this data:\n",
    "        {cuisine_data}\n",
    "        Unless prompted by the user, provide choices appropriate for this time: {time}\n",
    "\n",
    "        You treat them like a good friend.\"\n",
    "    \"\"\"\n",
    "\n",
    "    # use preaggregated values from user's rating history\n",
    "    cuisines = user_ratings_and_total_visits_by_cuisine_raw['cuisine_keys_1825d']\n",
    "    ratings = user_ratings_and_total_visits_by_cuisine_raw['users_average_rating_by_cuisine_last_5y']\n",
    "    visits = user_ratings_and_total_visits_by_cuisine_raw['users_total_visits_by_cuisine_last_5y']\n",
    "    \n",
    "    # format the ratings for the prompt\n",
    "    cuisine_ratings = []\n",
    "    for cuisine, rating, visit in zip(cuisines, ratings, visits):\n",
    "        cuisine_ratings.append({\n",
    "            'cuisine': cuisine,\n",
    "            'average_rating': rating,\n",
    "            'total_visits': visit\n",
    "        })\n",
    "    \n",
    "    #build final prompt with location, time and cuisine context\n",
    "    prompt_with_context = system_prompt_template.format(\n",
    "        location = user_recommendation_request[\"location\"],\n",
    "        time = datetime.now().strftime(\"%H:%M\"),\n",
    "        cuisine_data = str(cuisine_ratings)\n",
    "    )\n",
    "\n",
    "    return {\"system_prompt\": prompt_with_context}\n",
    "\n",
    "restaurant_recommender_prompt_v2.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lineage\n",
    "\n",
    "You can take a look at the Tecton UI to see the different metadata and pipelines of each version of the prompt at:\n",
    "\n",
    "[Pipeline view for prompt `restaurant_recommender_prompt:v1`](https://community.tecton.ai/app/repo/gen-ai-prompt/feature-services/prompt_restaurant_recommender%3Av1/pipeline)\n",
    "Screenshot:\n",
    "![](./prompt-v1-pipeline.png)\n",
    "The V1 pipelines shows the dependency on the real-time request data source, the transformations it uses to produce the context enriched prompt and the online/batch service that serves it.\n",
    "\n",
    "\n",
    "[Pipeline view for prompt `restaurant_recommender_prompt:v2`](https://community.tecton.ai/app/repo/gen-ai-prompt/feature-services/prompt_restaurant_recommender%3Av2/pipeline)\n",
    "Screenshot:\n",
    "![](./prompt-v2-pipeline.png)\n",
    "The V2 pipelines starts earlier, showing the retaurants and user ratings activity as sources that are transformed into 3 features that keep the last 5 years average rating by cuisine, it also shows the dependency on the real-time request data source, the transformations it uses to produce the context enriched prompt from both the batch and the request time data source and the online/batch service that serves it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup tecton_client for feature retrieval\n",
    "\n",
    "You will need to \n",
    "- create a [Tecton client API Key by clicking \"Create Service Account\" here](https://community.tecton.ai/app/settings/accounts-and-access/service-accounts) \n",
    "- use it in the following cell replacing \"your-api-key\" for your key.\n",
    "\n",
    "The following cells install \"tecton_client\" package and instantiate a `tecton_client` object for retrieval from a feature service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tecton_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton_client import TectonClient\n",
    "\n",
    "tecton_client = TectonClient(url=\"https://community.tecton.ai/\", \n",
    "                             api_key = \"your-api-key\"  # REPLACE WITH YOUR TECTON API KEY\n",
    "                             default_workspace_name=\"gen-ai-prompt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Restaurant Recommendation App consuming from the Feature Service\n",
    "\n",
    "The following cell simulates the restaurant recommender app by identifying the `user_entity`, and providing the location information in real-time through `app_data`. It uses the `tecton_client` to request online features from the feature service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate a specific user of the app \n",
    "user_id = '953c6db5-89ea-4189-b286-c662591487c8'\n",
    "user_entity = {'user_id': user_id}\n",
    "\n",
    "# simulate request data from mobile device GPS location\n",
    "app_data = {\"location\" : \"latitude=35.0,longitude=-80.0\"}  \n",
    "\n",
    "features = tecton_client.get_features(\n",
    "    feature_service_name=\"prompt_restaurant_recommender:v2\",\n",
    "    join_key_map=user_entity,\n",
    "    request_context_map=app_data,\n",
    ").get_features_dict()\n",
    "\n",
    "system_prompt = features['restaurant_recommender_prompt:v2.system_prompt']\n",
    "print(\"DYNAMIC SYSTEM PROMPT:\\n================================================\\n\")\n",
    "print(system_prompt)\n",
    "print(\"\\n================================================\\n CHAT RESPONSE:\\n\")\n",
    "\n",
    "response = ai_chat(\"what should I eat?\", system_prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It provides a recommendation based on the cuisine data that we provided. Notice that if the user asks for a specific cuisine, the response changes accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ai_chat(\"I feel like eating Indian food, where should I eat?\", prompt['system_prompt'])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "#testing with time travel\n",
    "chat_events = [\n",
    "        {\"timestamp\":datetime.strptime(\"2021-10-01 09:00:01-05:00\",\"%Y-%m-%d %H:%M:%S%z\"),\"user_id\":user_id, \"user_message\": \"where should I eat?\", \"location\":\"latitude=35.0,longitude=-80.0\"},\n",
    "        {\"timestamp\":datetime.strptime(\"2022-10-01 09:00:01-05:00\",\"%Y-%m-%d %H:%M:%S%z\"),\"user_id\":user_id, \"user_message\": \"where should I eat?\", \"location\":\"latitude=35.0,longitude=-80.0\"},\n",
    "        {\"timestamp\":datetime.strptime(\"2023-10-01 09:00:01-05:00\",\"%Y-%m-%d %H:%M:%S%z\"),\"user_id\":user_id, \"user_message\": \"where should I eat?\", \"location\":\"latitude=35.0,longitude=-80.0\"},\n",
    "        ]\n",
    "events_df = pd.DataFrame(chat_events)\n",
    "\n",
    "ws = tecton.get_workspace(\"gen-ai-prompt\")\n",
    "fs = ws.get_feature_service(\"prompt_restaurant_recommender:v2\")\n",
    "batch_data = fs.get_features_for_events(events_df, timestamp_key=\"timestamp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in batch_data.to_pandas()[\"restaurant_recommender_prompt:v2__system_prompt\"]:\n",
    "    print (prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Context enriched prompts improve generative AI applications by providing relevant context. There are many benefits to using the Tecton platform to develop and manage prompts, \n",
    "- Bring standardization and DevOps best practices to your mission critical prompts.\n",
    "- Enrich your prompts with features for personalized LLM responses.\n",
    "- Unlock fine tuning and backtesting with historical prompt accuracy.\n",
    "- Enable versioning, discoverability, and lineage for prompts.\n",
    "- Iterate rapidly and A/B test prompts without depending on application engineers.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycon-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
