{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features as Tools \n",
    "\n",
    "This tutorial guides you through creating an LLM generated restaurant recommendation function by using Features as Tools.\n",
    "\n",
    "This is an example of how an LLM can make use of feature views as data retrieval tools. At development time, you provide the set of feature views that the LLM agent is allowed to call. The LLM uses the feature view's description field to determine what tools are relevant for answering the user question. It  makes the corresponing feature retrieval call(s) using the Tecton entity keys provided in the context of the call.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'tecton[rift]==0.10.0b32' gcsfs s3fs --force-reinstall\n",
    "!pip install \"$HOME/Downloads/tecton_gen_ai-0.0.4-py3-none-any.whl\" --force-reinstall\n",
    "\n",
    "!pip install openai\n",
    "!pip install langchain-openai\n",
    "!pip install langchain\n",
    "!pip install langchain_community\n",
    "!pip install langchain_core\n",
    "!pip install llama-index\n",
    "!pip install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features as Tools\n",
    "\n",
    "In the following cell you'll create a Tecton Agent with a simple prompt and two different feature views for the LLM as tools.\n",
    "The system prompt in this example, instructs the LLM to address the user by name and to only suggest restaurants that in near the location. In order to address the use by name the LLM will need to use a tools.\n",
    "The location is configured to be provided as part of the context in the request by using a RequestSource object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton_gen_ai.agent import AgentService, AgentClient\n",
    "from tecton_gen_ai import prompt\n",
    "from tecton_gen_ai.core_utils import make_request_source\n",
    "\n",
    "def restaurant_recommender_agent( user_info_fv, recent_visit_fv):    \n",
    "\n",
    "    location_request = make_request_source(location = str)\n",
    "    \n",
    "    @prompt(sources=[ location_request])\n",
    "    def sys_prompt(location_request ):\n",
    "        location = location_request[\"location\"]\n",
    "        return f\"\"\"\n",
    "        Address the user by name.\n",
    "        You are a consierge service that recommends restaurants.\n",
    "        Respond to the user's question. \n",
    "        If the user asks for a restaurant recommendation respond with a specific restaurant that you know and suggested menu items. \n",
    "        Only suggest restaurants that are in or near {location}. \n",
    "        \"\"\"\n",
    "            \n",
    "    return AgentService(\n",
    "        name=\"restaurant_recommender\",\n",
    "        prompts=[sys_prompt],\n",
    "        tools=[ user_info_fv, recent_visit_fv],  # feature views used as tools\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sys_prompt` instructions above require the LLM to \"address the user by name\" which requires the use of a tool.\n",
    " \n",
    "In the next cell we create a couple of mock feature views that provide basic user information and the list of their recently visited restaurants respectively.\n",
    "\n",
    "In practice, these feature views would be implemented as stream feature views that would provide the latest user information and recent visits within seconds of the corresponding user activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tecton import RequestSource\n",
    "from tecton.types import Field, String\n",
    "\n",
    "\n",
    "from tecton_utils.testing import mock_batch_feature_view\n",
    "\n",
    "\n",
    "# mock user info data\n",
    "user_info = pd.DataFrame(\n",
    "        [\n",
    "            {\"user_id\": \"user1\", \"name\": \"Jim\",  \"age\": 30, \"food_preference\": \"American\"},\n",
    "            {\"user_id\": \"user2\", \"name\": \"John\", \"age\": 40, \"food_preference\": \"Italian\"},\n",
    "            {\"user_id\": \"user3\", \"name\": \"Jane\", \"age\": 50, \"food_preference\": \"Chinese\"},\n",
    "        ]\n",
    "    )\n",
    "user_info_fv = mock_batch_feature_view( \"user_info_fv\", user_info, entity_keys=[\"user_id\"], description=\"User's basic information.\")\n",
    "\n",
    "# mock user's recent visits \n",
    "recent_eats = pd.DataFrame(\n",
    "        [\n",
    "            {\"user_id\": \"user1\", \"last_3_visits\":str([\"Mama Ricotta's\", \"The Capital Grille\", \"Firebirds Wood Fired Grill\"])},\n",
    "            {\"user_id\": \"user2\", \"last_3_visits\":str([\"Mama Ricotta's\", \"Villa Antonio\", \"Viva Chicken\"])},\n",
    "            {\"user_id\": \"user3\", \"last_3_visits\":str([\"Wan Fu\", \"Wan Fu Quality Chinese Cuisine\", \"Ru San's\"])},\n",
    "        ]\n",
    "    )\n",
    "recent_eats_fv = mock_batch_feature_view( \"recent_eats_fv\", recent_eats, entity_keys=[\"user_id\"], description=\"User's recent restaurant visits.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature views entity key in both cases is `user_id`, the key will need to be provided in the context when invoking the LLM agent. \n",
    "\n",
    "In the following cell, you create a Tecton AgentClient with uses the mock feature views defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Tecton Agent\n",
    "recommender_agent = restaurant_recommender_agent( user_info_fv, recent_eats_fv )\n",
    "\n",
    "# create a client to invoke the agent\n",
    "client = AgentClient.from_local( recommender_agent )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it all together\n",
    "\n",
    "The Tecton AgentClient created above can be used to create a LangChain or LlamaIndex runnable agent.\n",
    "\n",
    "In the cell below you will instantiate an LLM model using OpenAI's `gpt-4o-mini` model and create a LangChain agent that is ready to use the tools we've provided above.\n",
    "\n",
    "Obtain an [OpenAI API key](https://platform.openai.com/api-keys) and replace \"your-openai-key\" in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai as oa\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# replace with your key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
    "\n",
    "# instantiate LLM model for both LangChain & LlamaIndex showing same integration with both\n",
    "langchain_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llamaindex_llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# create invokable agents for LangChain & LlamaIndex \n",
    "langchain_agent = client.make_agent(langchain_llm, system_prompt=\"sys_prompt\")\n",
    "llamaindex_agent = client.make_agent(llamaindex_llm, system_prompt=\"sys_prompt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with different context\n",
    "\n",
    "In the following cells you'll provide different context and invoke the LLM agent to test how features as tools provide user specifc personalization, you'll notice that we can either use the langchain or llamaindex agents created above with the same results. \n",
    "\n",
    "Notice that the function name and signature vary between the two because they langchain and llamaindex invoke methods are different and have different input/ouput signatures.\n",
    "\n",
    "The first test uses the `langchain_agent`, it's input and output are a dictionaries with \"input\" and \"output\" keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context - user 1 in Charlotte \n",
      "\n",
      "Response:\n",
      "Hello Jim! Based on your recent visits, it seems you enjoy American cuisine. \n",
      "\n",
      "I recommend trying **The Palm** in Ballantyne. It's a classic American steakhouse known for its exceptional service and delicious food. \n",
      "\n",
      "**Menu suggestions:**\n",
      "- **Filet Mignon** - Tender and cooked to perfection.\n",
      "- **Lobster Mac & Cheese** - A rich and creamy side that's a crowd favorite.\n",
      "- **Chocolate Cake** - A must-try for dessert; it's decadent and satisfying.\n",
      "\n",
      "Enjoy your dining experience!\n"
     ]
    }
   ],
   "source": [
    "print(\"Context - user 1 in Charlotte \\n\\nResponse:\")\n",
    "with client.set_context({\"user_id\": \"user1\", \"location\":\"Ballantyne, Charlotte, NC\"}):\n",
    "    print(langchain_agent.invoke({\"input\":\"what do you know about me and what would you recommend\"})[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it out\n",
    "\n",
    "In the following cells you can see how the response changes based on the `user_id` and the `location` provided resulting in a personalized response for each user and based on their current location.\n",
    "\n",
    "The following test uses the `llamaindex_agent` with its `chat` method that receiving a query string as input and returns output as a string as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context - user 2 in Charlotte \n",
      "\n",
      "Response:\n",
      "Hi John! Another great option for Italian cuisine that you haven't tried yet is **Maggiano's Little Italy** in the nearby area. \n",
      "\n",
      "They have a wonderful selection of classic Italian dishes. You might want to try their **Fettuccine Alfredo** or the **Lasagna**. For an appetizer, the **Stuffed Mushrooms** are highly recommended, and for dessert, their **Tiramisu** is a must-try!\n",
      "\n",
      "Enjoy your evening out!\n"
     ]
    }
   ],
   "source": [
    "print( \"Context - user 2 in Charlotte \\n\\nResponse:\")\n",
    "with client.set_context({\"user_id\": \"user2\", \"location\":\"Ballantyne, Charlotte, NC\"}):\n",
    "    print(llamaindex_agent.chat(\"I want to try a place tonight that I've never been to that also fits my preference\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context - user 3 in Charlotte \n",
      "\n",
      "Response:\n",
      "I recommend going back to **Mama Ricotta's**! It's a wonderful Italian restaurant where you can enjoy their delicious **Lasagna** or the **Pasta Carbonara**. \n",
      "\n",
      "For an appetizer, the **Bruschetta** is a great choice, and if you have room for dessert, their **Tiramisu** is a must-try. Enjoy your meal!\n"
     ]
    }
   ],
   "source": [
    "print( \"Context - user 3 in Charlotte \\n\\nResponse:\")\n",
    "with client.set_context({\"user_id\": \"user3\", \"location\":\"Charlotte, NC\"}):\n",
    "    print(llamaindex_agent.chat(\"Recommend one of my regular spots for dinner\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context - user 3 in Charlotte \n",
      "\n",
      "Response:\n",
      "I recommend going back to **Mama Ricotta's**! It's a wonderful Italian restaurant where you can enjoy their delicious **Lasagna** or the **Pasta Carbonara**. \n",
      "\n",
      "For an appetizer, the **Bruschetta** is a great choice, and if you have room for dessert, their **Tiramisu** is a must-try. Enjoy your meal!\n"
     ]
    }
   ],
   "source": [
    "print( \"Context - user 3 in Charlotte \\n\\nResponse:\")\n",
    "with client.set_context({\"user_id\": \"user3\", \"location\":\"Charlotte, NC\"}):\n",
    "    print(langchain_agent.chat(\"Recommend one of my regular spots for dinner\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Tecton delivers real-time, streaming and batch feature pipelines in production. Enhancing an LLMs context through features as tools, brings the power of fresh context to generative AI applications. By using features as tools for the LLM, you give the LLM the ability to retrieve relevant information only when the user question needs requires it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tecton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
