{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features as Tools \n",
    "\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/tecton-ai/gen-ai/blob/main/features-as-tools.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" width=\"150\"/>\n",
    "</a>\n",
    "\n",
    "This tutorial guides you through creating an LLM generated restaurant recommendation function by using Features as Tools.\n",
    "\n",
    "This is an example of how an LLM can make use of feature views as data retrieval tools. At development time, you provide the set of feature views that the LLM agent is allowed to call. The LLM uses the feature view's description field to determine what tools are relevant for answering the user question. It  makes the corresponding feature retrieval call(s) using the Tecton entity keys provided in the context of the call.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = !pip install 'tecton-gen-ai[tecton,langchain,llama-index,dev]' langchain-openai llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton_gen_ai.testing import set_dev_mode\n",
    "\n",
    "set_dev_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features as Tools\n",
    "\n",
    "In the following cell you'll create a Tecton Agent with a simple prompt and two different feature views for the LLM as tools.\n",
    "The system prompt in this example, instructs the LLM to address the user by name and to only suggest restaurants that in near the location. In order to address the use by name the LLM will need to use a tools.\n",
    "The location is configured to be provided as part of the context in the request by using a RequestSource object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton_gen_ai.fco import prompt, AgentService, AgentClient\n",
    "from tecton_gen_ai.utils.tecton_utils import make_request_source\n",
    "\n",
    "def restaurant_recommender_agent( user_info_fv, recent_visit_fv):    \n",
    "\n",
    "    location_request = make_request_source(location = str)\n",
    "    \n",
    "    @prompt(sources=[ location_request])\n",
    "    def sys_prompt(location_request ):\n",
    "        location = location_request[\"location\"]\n",
    "        return f\"\"\"\n",
    "        Address the user by name.\n",
    "        You are a consierge service that recommends restaurants.\n",
    "        Respond to the user's question. \n",
    "        If the user asks for a restaurant recommendation respond with a specific restaurant that you know and suggested menu items. \n",
    "        Only suggest restaurants that are in or near {location}. \n",
    "        \"\"\"\n",
    "            \n",
    "    return AgentService(\n",
    "        name=\"restaurant_recommender\",\n",
    "        prompts=[sys_prompt],\n",
    "        tools=[ user_info_fv, recent_visit_fv],  # feature views used as tools\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sys_prompt` instructions above require the LLM to \"address the user by name\" which requires the use of a tool.\n",
    " \n",
    "In the next cell we create a couple of mock feature views that provide basic user information and the list of their recently visited restaurants respectively.\n",
    "\n",
    "In practice, these feature views would be implemented as stream feature views that would provide the latest user information and recent visits within seconds of the corresponding user activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tecton_gen_ai.testing import make_local_batch_feature_view\n",
    "\n",
    "\n",
    "# mock user info data\n",
    "user_info = [\n",
    "    {\"user_id\": \"user1\", \"name\": \"Jim\",  \"age\": 30, \"food_preference\": \"American\"},\n",
    "    {\"user_id\": \"user2\", \"name\": \"John\", \"age\": 40, \"food_preference\": \"Italian\"},\n",
    "    {\"user_id\": \"user3\", \"name\": \"Jane\", \"age\": 50, \"food_preference\": \"Chinese\"},\n",
    "]\n",
    "user_info_fv = make_local_batch_feature_view(\n",
    "    \"user_info_fv\",\n",
    "    user_info,\n",
    "    [\"user_id\"],\n",
    "    description=\"User's basic information.\"\n",
    ")\n",
    "\n",
    "# mock user's recent visits \n",
    "recent_eats = [\n",
    "    {\"user_id\": \"user1\", \"last_3_visits\":str([\"Mama Ricotta's\", \"The Capital Grille\", \"Firebirds Wood Fired Grill\"])},\n",
    "    {\"user_id\": \"user2\", \"last_3_visits\":str([\"Mama Ricotta's\", \"Villa Antonio\", \"Viva Chicken\"])},\n",
    "    {\"user_id\": \"user3\", \"last_3_visits\":str([\"Wan Fu\", \"Wan Fu Quality Chinese Cuisine\", \"Ru San's\"])},\n",
    "]\n",
    "recent_eats_fv = make_local_batch_feature_view(\n",
    "    \"recent_eats_fv\",\n",
    "    recent_eats,\n",
    "    entity_keys=[\"user_id\"],\n",
    "    description=\"User's recent restaurant visits.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature views entity key in both cases is `user_id`, the key will need to be provided in the context when invoking the LLM agent. \n",
    "\n",
    "In the following cell, you create a Tecton AgentClient with uses the mock feature views defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Tecton Agent\n",
    "recommender_agent = restaurant_recommender_agent( user_info_fv, recent_eats_fv )\n",
    "\n",
    "# create a client to invoke the agent\n",
    "client = AgentClient.from_local( recommender_agent )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it all together\n",
    "\n",
    "The Tecton AgentClient created above can be used to create a LangChain or LlamaIndex runnable agent.\n",
    "\n",
    "In the cell below you will instantiate an LLM model using OpenAI's `gpt-4o-mini` model and create a LangChain agent that is ready to use the tools we've provided above. In the following section, you'll do the same using a LlamaIndex.\n",
    "\n",
    "Obtain an [OpenAI API key](https://platform.openai.com/api-keys) and replace \"your-openai-key\" in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from tecton_gen_ai.testing.utils import print_md\n",
    "\n",
    "\n",
    "# replace with your key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
    "\n",
    "# instantiate LLM model for  LangChain \n",
    "langchain_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# create invocable agent for LangChain \n",
    "langchain_agent = client.make_agent(langchain_llm, system_prompt=\"sys_prompt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with different context\n",
    "\n",
    "In the following cells you'll provide different context and invoke the LLM agent to test how features as tools provide user specific personalization. \n",
    "\n",
    "The first test uses the `langchain_agent`, it's input and output are a dictionaries with \"input\" and \"output\" keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context - user 1 in Charlotte \n",
      "\n",
      "Response:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Hi Jim! Based on your recent visits, it looks like you enjoy American cuisine. I recommend trying <span style=\"font-weight: bold\">The Cheesecake </span>  \n",
       "<span style=\"font-weight: bold\">Factory</span> in Ballantyne. They have a diverse menu, but here are some specific items you might enjoy:                 \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Bang Bang Chicken and Shrimp</span>: A delicious, spicy dish with crispy chicken and shrimp.                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Factory Burrito Grande</span>: A massive burrito filled with your choice of protein and topped with sour cream and     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>guacamole.                                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Godiva® Cheesecake</span>: For dessert, you can't go wrong with this rich and indulgent cheesecake.                    \n",
       "\n",
       "Enjoy your meal!                                                                                                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Hi Jim! Based on your recent visits, it looks like you enjoy American cuisine. I recommend trying \u001b[1mThe Cheesecake \u001b[0m  \n",
       "\u001b[1mFactory\u001b[0m in Ballantyne. They have a diverse menu, but here are some specific items you might enjoy:                 \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mBang Bang Chicken and Shrimp\u001b[0m: A delicious, spicy dish with crispy chicken and shrimp.                           \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mFactory Burrito Grande\u001b[0m: A massive burrito filled with your choice of protein and topped with sour cream and     \n",
       "\u001b[1;33m   \u001b[0mguacamole.                                                                                                      \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mGodiva® Cheesecake\u001b[0m: For dessert, you can't go wrong with this rich and indulgent cheesecake.                    \n",
       "\n",
       "Enjoy your meal!                                                                                                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Context - user 1 in Charlotte \\n\\nResponse:\")\n",
    "with client.set_context({\"user_id\": \"user1\", \"location\":\"Ballantyne, Charlotte, NC\"}):\n",
    "    print_md(langchain_agent.invoke({\"input\":\"what do you know about me and what would you recommend\"})[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Context for Different Users\n",
    "\n",
    "In the following cells you can see how the response changes based on the `user_id` and the `location` provided resulting in a personalized response for each user and based on their current location.\n",
    "\n",
    "The following test uses the `llamaindex_agent` with its `chat` method that receiving a query string as input and returns output as a string as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context - user 2 in Charlotte \n",
      "\n",
      "Response:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Hi John! Since you're in the mood for Italian and want to try something new, I recommend checking out <span style=\"font-weight: bold\">Bistro La Bon</span>\n",
       "in Ballantyne.                                                                                                     \n",
       "\n",
       "You might enjoy their <span style=\"font-weight: bold\">Pasta Primavera</span> or the <span style=\"font-weight: bold\">Osso Buco</span>, and don’t forget to try their delicious <span style=\"font-weight: bold\">Tiramisu</span> for       \n",
       "dessert! It’s a cozy place with a great atmosphere. Enjoy your dinner!                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Hi John! Since you're in the mood for Italian and want to try something new, I recommend checking out \u001b[1mBistro La Bon\u001b[0m\n",
       "in Ballantyne.                                                                                                     \n",
       "\n",
       "You might enjoy their \u001b[1mPasta Primavera\u001b[0m or the \u001b[1mOsso Buco\u001b[0m, and don’t forget to try their delicious \u001b[1mTiramisu\u001b[0m for       \n",
       "dessert! It’s a cozy place with a great atmosphere. Enjoy your dinner!                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print( \"Context - user 2 in Charlotte \\n\\nResponse:\")\n",
    "with client.set_context({\"user_id\": \"user2\", \"location\":\"Ballantyne, Charlotte, NC\"}):\n",
    "    print_md(langchain_agent.invoke({\"input\":\"I want to try a place tonight that I've never been to that also fits my preference\"})[\"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context - user 3 in Charlotte \n",
      "\n",
      "Response:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Hi Jane! I see that you've been visiting some great spots. Since you enjoy Chinese cuisine, I recommend trying <span style=\"font-weight: bold\">Wan </span>\n",
       "<span style=\"font-weight: bold\">Fu Quality Chinese Cuisine</span> if you haven't been there recently. Their <span style=\"font-weight: bold\">Kung Pao Chicken</span> and <span style=\"font-weight: bold\">Peking Duck</span> are          \n",
       "particularly delicious. Enjoy your dinner!                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Hi Jane! I see that you've been visiting some great spots. Since you enjoy Chinese cuisine, I recommend trying \u001b[1mWan \u001b[0m\n",
       "\u001b[1mFu Quality Chinese Cuisine\u001b[0m if you haven't been there recently. Their \u001b[1mKung Pao Chicken\u001b[0m and \u001b[1mPeking Duck\u001b[0m are          \n",
       "particularly delicious. Enjoy your dinner!                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print( \"Context - user 3 in Charlotte \\n\\nResponse:\")\n",
    "with client.set_context({\"user_id\": \"user3\", \"location\":\"Charlotte, NC\"}):\n",
    "    print_md(langchain_agent.invoke({\"input\":\"Recommend one of my regular spots for dinner\"})[\"output\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With LlamaIndex \n",
    "\n",
    "Tecton also integrates with LlamaIndex in the same way it does with LangChain in order to provide enriched prompts and features as tools.\n",
    "\n",
    "The only difference is in the LangChain`.invoke` vs LlamaIndex`.chat` methods and their signatures. Tecton delivers fresh context in the same way for both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context - user 3 in Charlotte \n",
      "\n",
      "Response:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">It looks like you've recently visited Wan Fu and Ru San's. If you're in the mood for some delicious sushi, I       \n",
       "recommend heading back to Ru San's. Their spicy tuna roll and the salmon sashimi are particularly popular! Enjoy   \n",
       "your dinner!                                                                                                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "It looks like you've recently visited Wan Fu and Ru San's. If you're in the mood for some delicious sushi, I       \n",
       "recommend heading back to Ru San's. Their spicy tuna roll and the salmon sashimi are particularly popular! Enjoy   \n",
       "your dinner!                                                                                                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# instantiate LLM model for with LlamaIndex integration \n",
    "llamaindex_llm = OpenAI(model=\"gpt-4o-mini-2024-07-18\")\n",
    "\n",
    "# create invocable agent for LlamaIndex \n",
    "llamaindex_agent = client.make_agent(llamaindex_llm, system_prompt=\"sys_prompt\")\n",
    "\n",
    "print( \"Context - user 3 in Charlotte \\n\\nResponse:\")\n",
    "with client.set_context({\"user_id\": \"user3\", \"location\":\"Charlotte, NC\"}):\n",
    "    print_md(llamaindex_agent.chat(\"Recommend one of my regular spots for dinner\").response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the LlamaIndex case, given that it is stateful, a new instance of the agent is needed if the context is different between calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context - user 2 in New York \n",
      "\n",
      "Response:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">John, for a romantic dinner in New York, I recommend <span style=\"font-weight: bold\">Carbone</span>. It's an upscale Italian restaurant known for its     \n",
       "classic dishes and intimate atmosphere.                                                                            \n",
       "\n",
       "You might enjoy their <span style=\"font-weight: bold\">Spicy Rigatoni Vodka</span>, which is a fan favorite, and the <span style=\"font-weight: bold\">Veal Parmesan</span> is a must-try as well.  \n",
       "Don't forget to finish off with their delicious <span style=\"font-weight: bold\">Tiramisu</span> for dessert. Enjoy your evening!                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "John, for a romantic dinner in New York, I recommend \u001b[1mCarbone\u001b[0m. It's an upscale Italian restaurant known for its     \n",
       "classic dishes and intimate atmosphere.                                                                            \n",
       "\n",
       "You might enjoy their \u001b[1mSpicy Rigatoni Vodka\u001b[0m, which is a fan favorite, and the \u001b[1mVeal Parmesan\u001b[0m is a must-try as well.  \n",
       "Don't forget to finish off with their delicious \u001b[1mTiramisu\u001b[0m for dessert. Enjoy your evening!                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create new instance of agent for LlamaIndex \n",
    "llamaindex_agent = client.make_agent(llamaindex_llm, system_prompt=\"sys_prompt\")\n",
    "\n",
    "print( \"Context - user 2 in New York \\n\\nResponse:\")\n",
    "with client.set_context({\"user_id\": \"user2\", \"location\":\"New York, NY\"}):\n",
    "    print_md(llamaindex_agent.chat(\"something romantic for dinner\").response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Tecton delivers real-time, streaming and batch feature pipelines in production. Enhancing an LLMs context through features as tools, brings the power of fresh context to generative AI applications. By using features as tools for the LLM, you give the LLM the ability to retrieve relevant information only when the user question needs requires it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tecton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
